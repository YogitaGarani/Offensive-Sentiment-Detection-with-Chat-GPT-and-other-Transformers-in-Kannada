{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6076119,"sourceType":"datasetVersion","datasetId":3478107},{"sourceId":6228507,"sourceType":"datasetVersion","datasetId":3577624},{"sourceId":6402537,"sourceType":"datasetVersion","datasetId":3691582}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:22:41.000762Z","iopub.execute_input":"2023-09-08T14:22:41.001109Z","iopub.status.idle":"2023-09-08T14:22:41.012996Z","shell.execute_reply.started":"2023-09-08T14:22:41.001078Z","shell.execute_reply":"2023-09-08T14:22:41.011895Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\n!pip install langdetect\nfrom langdetect import detect\nwarnings.filterwarnings(\"ignore\")\n!pip install clean-text\nfrom cleantext import clean\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:22:41.015006Z","iopub.execute_input":"2023-09-08T14:22:41.015936Z","iopub.status.idle":"2023-09-08T14:23:16.352134Z","shell.execute_reply.started":"2023-09-08T14:22:41.015902Z","shell.execute_reply":"2023-09-08T14:23:16.350989Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=8e512132d3b8378bba5865cf63cfafa977d0ac68ebe91620138df33a75d1e4eb\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\nCollecting clean-text\n  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\nCollecting emoji<2.0.0,>=1.0.0 (from clean-text)\n  Downloading emoji-1.7.0.tar.gz (175 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy<7.0,>=6.0 (from clean-text)\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\nBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=e817b1e99d871438f686ba7fa89d53826fbd7344866c6bd8b746c979e6e8d7f7\n  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\nSuccessfully built emoji\nInstalling collected packages: emoji, ftfy, clean-text\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.6.0\n    Uninstalling emoji-2.6.0:\n      Successfully uninstalled emoji-2.6.0\nSuccessfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"def read(url):\n    df = pd.read_csv(url)\n    return df\ntrain = read('/kaggle/input/augmentedk/augmented_dataset/train_oversample.csv')\nval = read('/kaggle/input/offensivek/Datasets/Trans_kan_dev.csv')\ntest = read('/kaggle/input/offensivek/Datasets/Trans_kan_test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:23:16.354396Z","iopub.execute_input":"2023-09-08T14:23:16.354770Z","iopub.status.idle":"2023-09-08T14:23:16.570417Z","shell.execute_reply.started":"2023-09-08T14:23:16.354734Z","shell.execute_reply":"2023-09-08T14:23:16.569442Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nval = val.append(test)\ndef remnotk(offense):\n    if (offense == 'not-Kannada' or offense == 'label' or offense == 5):\n        return None\n    else:\n        return offense\ntrain['labels'] = train['labels'].apply(remnotk)\nval['label'] = val['label'].apply(remnotk)\n\n\nimport re\ndef clean_text(text):\n    text = re.sub(r\"@[^A-Za-z0-9]+\", '', text)\n    text = re.sub(r\"@[A-Za-z0-9]+\", '', text)\n    text = re.sub(r\"https?://[A-Za-z0-9./]+\", '', text)\n    #text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n    text = re.sub('\\t', '',  text)\n    text = re.sub(r\" +\", '', text)\n    return text    \ntrain['tweets'] = train['tweets'].apply(clean_text)\nval['transliterated'] = val['transliterated'].apply(clean_text)\n\n\ndef cleannotk(transliterated):\n    detected_language = detect(transliterated)\n    if (detected_language != 'kn'):\n        print(detected_language, transliterated)\n        return None\n    else:\n        return transliterated\nval['transliterated'] = val['transliterated'].apply(cleannotk)\ntrain['tweets'] = train['tweets'].apply(cleannotk)\n\n\ntrain = train.dropna()\ntrain = train.reset_index()\nprint(train)\nval = val.dropna()\nval = val.reset_index()\nprint(train)\n\ntrain['labels'] = train['labels']\ntrain['tweets'] = train['tweets']\n\nfor i in range(len(train)):\n    train['tweets'][i] = train['tweets'][i][8:-2]\ntrain = train.drop(columns = ['Unnamed: 0'])\nprint('Training dataset:\\n', train)\n\nval['labels'] = LabelEncoder().fit_transform( val['label'] )\nval['tweets'] = val['transliterated']\n\nfor i in range(len(val)):\n    val['tweets'][i] = val['tweets'][i][8:-2]\nval = val.drop(columns = ['label', 'Unnamed: 0', 'transliterated', 'Sentence'])\nprint('Training dataset:\\n', val)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:23:16.571696Z","iopub.execute_input":"2023-09-08T14:23:16.572050Z","iopub.status.idle":"2023-09-08T14:24:20.349282Z","shell.execute_reply.started":"2023-09-08T14:23:16.572002Z","shell.execute_reply":"2023-09-08T14:24:20.348235Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"sl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'2'}\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'1*50'}\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nhr {'kn':'u200d🤦\\u200d'}\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nml {'kn':'മലയാളീസ്ഹാൻഡ്\\u200cസ്അപ്പ്\\u200c'}\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nta {'kn':'மிகஅருமையானபாடல்.இசையும்மிகஅருமை.வாழ்த்துக்கள்இரக்ஷ்த்ஷெட்டி.'}\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nml {'kn':'മലയാളകൾഉണ്ടേൽലൈക്\\u200cഅടിച്ചിട്ട്പോടെ'}\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'46449449494'}\nsl {'kn':'24'}\nfi {'kn':'\nhi {'kn':'भाईहिंदीमेंकबआयेगी'}\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nhr {'kn':'ನನ್ನಾಗತುಬ್ಬಬಜಾರುಸಾರ್\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d'}\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nhi ಮರಡ್ಡಿहिन्दीऑडिएंसमेंभीಆನ್ಸ್कागजबक्रेजहै।जरूरदेखेंगेइसफिल्मको।ವೇವಿಲ್ಡಿಫಿನಿಟೆಲಿವಾಚ್ದಿಸ್ಮೂವಿ....\nta ಮಸ್ಟ್ವಾಚ್ಮದರ್ಸೆಂಟಿಮೆಂಟ್ವೀಡಿಯೋ..ನ್தாயின்அருமைதெரிந்தவர்கள்இதைதவறவிடாதீர்கள்.ನೀನ್\nml അടിപൊളിപടം..നായികകുട്ടൂസ്പൊളി\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nhi ರೆಕ್ಸ್कबतकहोसकतीहैरिलीज़\nte జైడిబాస్అభిమాని\nhi अरेवाह...गजबकीफीलहैइसगानेमें..?ಫ್ಯಾನ್ಫ್ರಮ್ಗುಜರಾತ್\nml ಸೂಪರ್....ಕೇರಳರಕ್ಷಿತ್ಶೆಟ್ಟಿಫ್ಯಾನ್ಸ್ಲೈಕ್ಮಾಡಿ.ലൈക്ക്അടിക്കൂസുഹൃത്തുക്കളെ...\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nfr ಹಂದಅಪ್\\u200d\\u200d\\u200d\nml സൂപ്പർമൂവി\nbg уσυяνι∂єσιѕяιಜಿಎнтвяσ\nml മലയാളീസ്ഉണ്ടെങ്കിൽಲೈಕ್ಬಟ್ಟನ್ಅಡಿಚ್പോട്ടികടಮಕ್ಕಳೆ..\nhi सरआपलोगोसेनिवेदनहैआपछत्तीसगढ़मध्य्प्रदेशइसतरफ़खुदकीटाकीजखरीदलीजियेजिसमेसिर्फसाउथकीडबमूवीहीचलेगीआपसेरिक्वेस्टहैआपपैसाइन्वेस्टकरकेदिखेछत्तीसगढ़दुर्गमेंतरुणबिगसिनेमाखालीहोरहाहैआपदुर्गमेंइन्वेस्टकरसकतेहैನ್फिरक्याआपलोगधीरेधीरेपूरेहरजिलोंमें1सिनेमाघरआपअलॉटकरतेजानाफिरदेखनाआपकीहरमूवीकोहमदेखसकतेहैऔरअच्छाबिजनेसभीकरसकेगी\nta ಮಸ್ಟ್ವಾಚ್ಮದರ್ಸೆಂಟಿಮೆಂಟ್ವೀಡಿಯೋ..ನ್தாயின்அருமைதெரிந்தவர்கள்இதைதவறவிடாதீர்கள்.ನೀನ್\nhi ರೆಕ್ಸ್कबतकहोसकतीहैरिलीज़\nhi हिन्दीमेंकबरिलीज़होगीइंतजारकररहेहैं\nhi सरआपलोगोसेनिवेदनहैआपछत्तीसगढ़मध्य्प्रदेशइसतरफ़खुदकीटाकीजखरीदलीजियेजिसमेसिर्फसाउथकीडबमूवीहीचलेगीआपसेरिक्वेस्टहैआपपैसाइन्वेस्टकरकेदिखेछत्तीसगढ़दुर्गमेंतरुणबिगसिनेमाखालीहोरहाहैआपदुर्गमेंइन्वेस्टकरसकतेहैನ್फिरक्याआपलोगधीरेधीरेपूरेहरजिलोंमें1सिनेमाघरआपअलॉटकरतेजानाफिरदेखनाआपकीहरमूवीकोहमदेखसकतेहैऔरअच्छाबिजनेसभीकरसकेगी\nml മലയാളീസ്ഉണ്ടെങ്കിൽಲೈಕ್ಬಟ್ಟನ್ಅಡಿಚ್പോട്ടികടಮಕ್ಕಳೆ..\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nta தியாஒருஅழகானகாதல்காவியம்.அனைத்துமொழியிலும்ஓர்தாக்கத்தைஏற்படுத்தியதிரைப்படம்.தமிழ்மக்களின்சார்பாகஎனதுமனமார்ந்தவாழ்த்துக்கள்.\nml മലയാളികൾഒണ്ടോഇവിടെ\nfr ಹಂದಅಪ್\\u200d\\u200d\\u200d\nml മലയാളീസ്ആരുംഇല്ലേ.ഈപടംഇതുവരെകാണാത്തമലയാളികൾലൈക്\nml കട്ടവെയ്റ്റിങ്മലയാളംಆನ್ಸ್\nml സൂപ്പർമൂവി\nta சூப்பர்சாங்க்அண்டுவிசுவல்எபக்ட்\nta ಮಸ್ಟ್ವಾಚ್ಮದರ್ಸೆಂಟಿಮೆಂಟ್ವೀಡಿಯೋ..ನ್தாயின்அருமைதெரிந்தவர்கள்இதைதவறவிடாதீர்கள்.ನೀನ್\nhi अरेवाह...गजबकीफीलहैइसगानेमें..?ಫ್ಯಾನ್ಫ್ರಮ್ಗುಜರಾತ್\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nml സൂപ്പർമൂവി\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nml മലയാളികൾഒണ്ടോഇവിടെ\nml രക്ഷിത്ഷെട്ടികേരളഫാൻസ്ನ್ರಾಕ್ಷಿತ್ಶೆಟ್ಟಿಕೇರಳಫ್ಯಾನ್ಸ್\nml മലയാളീസ്ഉണ്ടെങ്കിൽಲೈಕ್ಬಟ್ಟನ್ಅಡಿಚ್പോട്ടികടಮಕ್ಕಳೆ..\nml മലയാളീസ്ഉണ്ടെങ്കിൽಲೈಕ್ಬಟ್ಟನ್ಅಡಿಚ್പോട്ടികടಮಕ್ಕಳೆ..\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nml മലയാളീസ്ഉണ്ടെങ്കിൽಲೈಕ್ಬಟ್ಟನ್ಅಡಿಚ್പോട്ടികടಮಕ್ಕಳೆ..\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nmr ऊआहेकीनाहीನ್............ನ್हे.....हेही.हहह\nta ಮಸ್ಟ್ವಾಚ್ಮದರ್ಸೆಂಟಿಮೆಂಟ್ವೀಡಿಯೋ..ನ್தாயின்அருமைதெரிந்தவர்கள்இதைதவறவிடாதீர்கள்.ನೀನ್\nhi हिन्दीमेंकबरिलीज़होगीइंतजारकररहेहैं\nml കട്ടവെയ്റ്റിങ്മലയാളംಆನ್ಸ್\n       index  Unnamed: 0  labels  \\\n0          0           0     2.0   \n1          1           1     4.0   \n2          2           2     1.0   \n3          4           4     1.0   \n4          5           5     2.0   \n...      ...         ...     ...   \n24994  29994       29994     3.0   \n24995  29996       29996     4.0   \n24996  29997       29997     4.0   \n24997  29998       29998     3.0   \n24998  29999       29999     3.0   \n\n                                                  tweets  \n0             ನೀವುಕ್ಲಿಮಕ್ಸ್ನೋಡಿಲ್ಲಾಅದಕ್ಕೆನಿಂಗೆಇಷ್ಟಆಗಿಲ್ಲ  \n1                                          ಇದುಪಕ್ಕಸುಳ್ಳು  \n2           ಕಡುಒಡಿಯುವಮಾಜನೆಬೆರೆಮಿನ್ಸರ್ಟ್ಪೀಪಲ್ಗೆಟ್ಏಆನಸ್ವರ್  \n3      ಬರಿಪ್ರಯೋಜನವೇಬರದಫಿಲ್ಮಸ್ನೋಡೊದುಫ್ಯಾಸಿಕೋರ್ಯುಟ್ಯೂಬ್...  \n4                     ಎಲ್ಲನೂಜೆಬ್ಗೆಯಾಕ್ಕಂಪೇರ್ಮಾಡ್ತಿಯಾಗುರು  \n...                                                  ...  \n24994  ಟೂಆವರ್ಸೆಂಟಿಮೆಂಟ್ನಾಟಚ್ಮಾಡಿಕಾನ್ನೀರ್ಹಾಕ್ಸಿಅದನಕವರ್...  \n24995                           ಸೂಪರ್ಅನ್ನಾಸಕತಾಗಿಉಗ್ದಿಡಿರ  \n24996  ಕನ್ನಡದಲ್ಲಿಮಾಡೋಬದ್ಲುಹಿಂದಿನಲ್ಲಿಮಾಡಿದ್ರೆಸುಪರ್ರಿಟ್...  \n24997  ಏಜನತಾಶತ್ರುಕೂಡಾಟಿಕ್ಟಾಕ್ನಾಮಾತ್ರಾಯೂನಿನ್ಸ್ಟಲ್ಮದೋಳಅ...  \n24998                 ಮಾರ್ಕೆಟಿಂಗ್ಸ್ವಲ್ಪಬೇಕಿತ್ತುಅನ್ಸುತ್ತೆ  \n\n[24999 rows x 4 columns]\n       index  Unnamed: 0  labels  \\\n0          0           0     2.0   \n1          1           1     4.0   \n2          2           2     1.0   \n3          4           4     1.0   \n4          5           5     2.0   \n...      ...         ...     ...   \n24994  29994       29994     3.0   \n24995  29996       29996     4.0   \n24996  29997       29997     4.0   \n24997  29998       29998     3.0   \n24998  29999       29999     3.0   \n\n                                                  tweets  \n0             ನೀವುಕ್ಲಿಮಕ್ಸ್ನೋಡಿಲ್ಲಾಅದಕ್ಕೆನಿಂಗೆಇಷ್ಟಆಗಿಲ್ಲ  \n1                                          ಇದುಪಕ್ಕಸುಳ್ಳು  \n2           ಕಡುಒಡಿಯುವಮಾಜನೆಬೆರೆಮಿನ್ಸರ್ಟ್ಪೀಪಲ್ಗೆಟ್ಏಆನಸ್ವರ್  \n3      ಬರಿಪ್ರಯೋಜನವೇಬರದಫಿಲ್ಮಸ್ನೋಡೊದುಫ್ಯಾಸಿಕೋರ್ಯುಟ್ಯೂಬ್...  \n4                     ಎಲ್ಲನೂಜೆಬ್ಗೆಯಾಕ್ಕಂಪೇರ್ಮಾಡ್ತಿಯಾಗುರು  \n...                                                  ...  \n24994  ಟೂಆವರ್ಸೆಂಟಿಮೆಂಟ್ನಾಟಚ್ಮಾಡಿಕಾನ್ನೀರ್ಹಾಕ್ಸಿಅದನಕವರ್...  \n24995                           ಸೂಪರ್ಅನ್ನಾಸಕತಾಗಿಉಗ್ದಿಡಿರ  \n24996  ಕನ್ನಡದಲ್ಲಿಮಾಡೋಬದ್ಲುಹಿಂದಿನಲ್ಲಿಮಾಡಿದ್ರೆಸುಪರ್ರಿಟ್...  \n24997  ಏಜನತಾಶತ್ರುಕೂಡಾಟಿಕ್ಟಾಕ್ನಾಮಾತ್ರಾಯೂನಿನ್ಸ್ಟಲ್ಮದೋಳಅ...  \n24998                 ಮಾರ್ಕೆಟಿಂಗ್ಸ್ವಲ್ಪಬೇಕಿತ್ತುಅನ್ಸುತ್ತೆ  \n\n[24999 rows x 4 columns]\nTraining dataset:\n        index  labels                                             tweets\n0          0     2.0                   ಮಕ್ಸ್ನೋಡಿಲ್ಲಾಅದಕ್ಕೆನಿಂಗೆಇಷ್ಟಆಗಿಲ\n1          1     4.0                                                ುಳ್\n2          2     1.0                 ವಮಾಜನೆಬೆರೆಮಿನ್ಸರ್ಟ್ಪೀಪಲ್ಗೆಟ್ಏಆನಸ್ವ\n3          4     1.0  ಜನವೇಬರದಫಿಲ್ಮಸ್ನೋಡೊದುಫ್ಯಾಸಿಕೋರ್ಯುಟ್ಯೂಬ್ಲಿಕಾಮೆಂಟ...\n4          5     2.0                           ಬ್ಗೆಯಾಕ್ಕಂಪೇರ್ಮಾಡ್ತಿಯಾಗು\n...      ...     ...                                                ...\n24994  29994     3.0  ಂಟಿಮೆಂಟ್ನಾಟಚ್ಮಾಡಿಕಾನ್ನೀರ್ಹಾಕ್ಸಿಅದನಕವರ್ಫೋಟೋಮಾಡಿ...\n24995  29996     4.0                                     ನಾಸಕತಾಗಿಉಗ್ದಿಡ\n24996  29997     4.0          ಲಿಮಾಡೋಬದ್ಲುಹಿಂದಿನಲ್ಲಿಮಾಡಿದ್ರೆಸುಪರ್ರಿಟ್ಅಗೋ\n24997  29998     3.0  ರುಕೂಡಾಟಿಕ್ಟಾಕ್ನಾಮಾತ್ರಾಯೂನಿನ್ಸ್ಟಲ್ಮದೋಳಅನ್ಸುತೆಕೆ...\n24998  29999     3.0                           ಂಗ್ಸ್ವಲ್ಪಬೇಕಿತ್ತುಅನ್ಸುತ್\n\n[24999 rows x 3 columns]\nTraining dataset:\n       index  labels                                             tweets\n0         0       0                                         00ಡೇಸ್ಪಕ್ಕ\n1         3       0  ಾರಿನೀವುವೀಡಿಯೋನಾರೋಸ್ಟ್ಮಾಡಿಅದ್ರೆಮದ್ವಪಬ್ಗ್ಆಟವಾಫ್ರ...\n2         4       0                                  ೃಷಣಶಾಪತಯ್ತೆಲೀಬೇಕು\n3         6       0  ನ್ನಡಇವತ್ತುರಾಷ್ಟ್ರೀಯಮತ್ತುಅಂತಾರಾಷ್ಟ್ರೀಯಮಟ್ಟದಲ್ಲಿ...\n4         7       0  ೇಳಿಕಾದಿರುವಭಾಂದವರೇನ್ಭುವಿಯಲ್ಲಿಅವನಅರಿತವರೆನ್ಯಾರಿಲ್...\n...     ...     ...                                                ...\n1049    771       4                                       ನಗುಕನ್ನಡಸಾಕು\n1050    773       0  ್ಟರ್ಟಿಪ್ಸ್ಟಾರ್ಟ್ಮಾಡಲುಕ್ಯಾಪಿಟಿಗ್ರೌತ್ಮಾಡಲುಟಿಪ್ಸ್...\n1051    774       2                  ಶ್ಮಿಕಇದನ್ನನೋಡಿಥಿ*ಏಊರ್ಕಿಂಬೇಕು🤣🤣🤣🤣🤣\n1052    776       0                ೋಡ್ತಾಹೋದ್ರೆಸಾವಿರಾರುಲಿರಿಕಲ್ವೀಡಿಯೋಗಳು\n1053    777       0                ಖತ್ಟ್ರೋಲ್ಬ್ರದರ್ನ್ಬ್ರೊಈಗೇಮ್ನೇಮ್ಹೇಳಿ\n\n[1054 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"train = train.append(val)\ntrain = train.drop('index', axis = 1)\nprint(train['labels'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:24:20.352174Z","iopub.execute_input":"2023-09-08T14:24:20.352633Z","iopub.status.idle":"2023-09-08T14:24:20.367266Z","shell.execute_reply.started":"2023-09-08T14:24:20.352596Z","shell.execute_reply":"2023-09-08T14:24:20.366331Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"0.0    5815\n1.0    5080\n2.0    5071\n4.0    5058\n3.0    5029\nName: labels, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(train, test_size = 0.1)\nprint(val['labels'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:24:20.368731Z","iopub.execute_input":"2023-09-08T14:24:20.369381Z","iopub.status.idle":"2023-09-08T14:24:20.466146Z","shell.execute_reply.started":"2023-09-08T14:24:20.369339Z","shell.execute_reply":"2023-09-08T14:24:20.464719Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0.0    560\n1.0    517\n4.0    517\n2.0    515\n3.0    497\nName: labels, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"models = []\ntokenizers = []\nfrom transformers import AutoTokenizer, AutoModel, BertTokenizer, XLMRobertaTokenizer\nmodel_names = [\n    'xlm-roberta-base',\n    'bert-base-multilingual-cased',\n    'ai4bharat/indic-bert',\n    'l3cube-pune/kannada-bert'\n    \n]\ntokenizers = [\n    XLMRobertaTokenizer.from_pretrained('xlm-roberta-base'),\n    BertTokenizer.from_pretrained('bert-base-multilingual-cased'),\n    AutoTokenizer.from_pretrained('ai4bharat/indic-bert'),\n    AutoTokenizer.from_pretrained(\"l3cube-pune/kannada-bert\")\n]\n\nfor name in model_names:\n    model = AutoModel.from_pretrained(name)\n    model.eval()\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:24:20.467623Z","iopub.execute_input":"2023-09-08T14:24:20.467987Z","iopub.status.idle":"2023-09-08T14:25:27.559480Z","shell.execute_reply.started":"2023-09-08T14:24:20.467952Z","shell.execute_reply":"2023-09-08T14:25:27.558473Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0b209d378b475e9a3bcd869f06e67a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4796cf77e7542cb997d8b805aaf5c38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eaa2f903098423d8952584d9e814441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f15027512949418bf1ae9409938a76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c0559f9f39456fb0b6fd79064502a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7192d175d947dabc0ea9990d0fd0ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c10a9867f54ab7ad2db333e502e980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3afc7c15f4439f8651d4e4c312abc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a70fa509460146b89f1c7449425fbf37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d3e3899dc040a196f4dda00cacd6ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021b5473c52649d7bf887708ffd39e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b47b3977d44ca5b5ad80f2505e4343"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a38c229ba9f4669a7426bde2ce08b1c"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9eb009ba8549899ce1f3f7cde21f3e"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.bias', 'sop_classifier.classifier.bias', 'sop_classifier.classifier.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias']\n- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beac2b13c2734ee3a5395b49cc342973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/951M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa853679eed049a8bfbd3546ea0d0a94"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at l3cube-pune/kannada-bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at l3cube-pune/kannada-bert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"lin_dim = np.sum([1024 if 'large' in name else 768 for name in model_names])\nlin_dim","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.561034Z","iopub.execute_input":"2023-09-08T14:25:27.561406Z","iopub.status.idle":"2023-09-08T14:25:27.572643Z","shell.execute_reply.started":"2023-09-08T14:25:27.561372Z","shell.execute_reply":"2023-09-08T14:25:27.571729Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"3072"},"metadata":{}}]},{"cell_type":"code","source":"n_models = len(models)\nn_models","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.574106Z","iopub.execute_input":"2023-09-08T14:25:27.574686Z","iopub.status.idle":"2023-09-08T14:25:27.672475Z","shell.execute_reply.started":"2023-09-08T14:25:27.574650Z","shell.execute_reply":"2023-09-08T14:25:27.671510Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"code","source":"for model in models:\n    for param in model.parameters():\n        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.674206Z","iopub.execute_input":"2023-09-08T14:25:27.675121Z","iopub.status.idle":"2023-09-08T14:25:27.685660Z","shell.execute_reply.started":"2023-09-08T14:25:27.675086Z","shell.execute_reply":"2023-09-08T14:25:27.684476Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.689852Z","iopub.execute_input":"2023-09-08T14:25:27.690682Z","iopub.status.idle":"2023-09-08T14:25:27.697378Z","shell.execute_reply.started":"2023-09-08T14:25:27.690646Z","shell.execute_reply":"2023-09-08T14:25:27.696345Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"label_mapping = {\n    'Not_offensive': 0, \n    'Offensive_Targeted_Insult_Other': 1, \n    'Offensive_Targeted_Insult_Group': 2, \n    'Offensive_Untargetede': 3, \n    'Offensive_Targeted_Insult_Individual': 4\n}\n\n# Collecting Text and Labels\ntrain_batch_sentences = list(train['tweets'])\ntrain_batch_labels =  [x for x in train['labels']]\ndev_batch_sentences = list(val['tweets'])\ndev_batch_labels =  [x for x in val['labels']]\ntest_batch_sentences = list(val['tweets'])\ntest_batch_labels =  [x for x in val['labels']]\n\n\n# Convert to Tensor\ntrain_encodings = [tokenizer(train_batch_sentences, padding = 'max_length', truncation = True, max_length = 512, return_tensors = \"pt\") for tokenizer in tokenizers]\ntrain_labels = torch.tensor(train_batch_labels)\ndev_encodings = [tokenizer(dev_batch_sentences, padding = 'max_length', truncation = True, max_length = 512, return_tensors = \"pt\") for tokenizer in tokenizers]\ndev_labels = torch.tensor(dev_batch_labels)\ntest_encodings = [tokenizer(test_batch_sentences, padding = 'max_length', truncation = True, max_length = 512, return_tensors = \"pt\") for tokenizer in tokenizers]\ntest_labels = torch.tensor(test_batch_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.698823Z","iopub.execute_input":"2023-09-08T14:25:27.699365Z","iopub.status.idle":"2023-09-08T14:27:09.740045Z","shell.execute_reply.started":"2023-09-08T14:25:27.699330Z","shell.execute_reply":"2023-09-08T14:27:09.738969Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass kannada_Offensive_Dataset(Dataset):\n    def __init__(self, encodings, labels = None):\n        self.encodings = encodings\n        self.labels = labels\n        self.n_models = len(encodings)\n\n    def __getitem__(self, idx):\n        item = {}\n        for i in range(self.n_models):\n            item.update({key+'_'+str(i): torch.tensor(val[idx]) for key, val in self.encodings[i].items()})\n        item['index'] = idx\n        if self.labels != None:\n            item['labels'] = torch.tensor(self.labels[idx])\n        else:\n            item['labels'] = torch.tensor(1)\n        return item\n\n    def __len__(self):\n        return len(self.encodings[0]['input_ids'])\n\n# Defining Datasets\ntrain_dataset = kannada_Offensive_Dataset(train_encodings, train_labels)\nlen(train_dataset)\ndev_dataset = kannada_Offensive_Dataset(dev_encodings, dev_labels)\ntest_dataset = kannada_Offensive_Dataset(test_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.741731Z","iopub.execute_input":"2023-09-08T14:27:09.742108Z","iopub.status.idle":"2023-09-08T14:27:09.753296Z","shell.execute_reply.started":"2023-09-08T14:27:09.742073Z","shell.execute_reply":"2023-09-08T14:27:09.751200Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\n\n# Basic Fully-Connected (Linear => BatchNorm => ReLU)\nclass BasicFC(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicFC, self).__init__()\n        self.fc = nn.Linear(in_channels, out_channels, **kwargs)\n        self.bn = nn.BatchNorm1d(out_channels, eps = 0.001)\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = self.bn(x)\n        return F.relu(x, inplace = True)\n\nclass FusionNet(torch.nn.Module):\n    def __init__(self, D_in, H1, H2, H3, H4, D_out):\n        super(FusionNet, self).__init__()\n        self.linear1_1 = BasicFC(D_in, H1)\n        self.linear1_2 = BasicFC(H1, H2)\n        self.linear1_3 = BasicFC(H2, H3)\n        self.linear1_4 = BasicFC(H3, H4)\n        self.dp = nn.Dropout(0.1)\n        self.linear2 = torch.nn.Linear(H4, D_out, bias = False)\n\n    def forward(self, x):\n        h_relu_1 = self.linear1_1(x)\n        h_relu_2 = self.dp(self.linear1_2(h_relu_1))\n        h_relu_3 = self.dp(self.linear1_3(h_relu_2))\n        h_relu_4 = self.dp(self.linear1_4(h_relu_3))\n        y_pred = self.linear2(h_relu_4)\n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.754601Z","iopub.execute_input":"2023-09-08T14:27:09.755032Z","iopub.status.idle":"2023-09-08T14:27:09.770502Z","shell.execute_reply.started":"2023-09-08T14:27:09.754996Z","shell.execute_reply":"2023-09-08T14:27:09.769423Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\n## Loss Fn\nXE_loss_function = nn.CrossEntropyLoss(reduction = 'mean').float()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.771898Z","iopub.execute_input":"2023-09-08T14:27:09.772271Z","iopub.status.idle":"2023-09-08T14:27:09.794203Z","shell.execute_reply.started":"2023-09-08T14:27:09.772234Z","shell.execute_reply":"2023-09-08T14:27:09.793267Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class F1_Loss(nn.Module):\n    def __init__(self, epsilon = 1e-7, n_labels = 5):\n        super().__init__()\n        self.epsilon = epsilon\n        self.n_labels = n_labels\n        \n    def forward(self, y_pred, y_true,):\n        assert y_pred.ndim == 2\n        assert y_true.ndim == 1\n        y_true = F.one_hot(y_true, self.n_labels).to(torch.float32)\n        y_pred = F.softmax(y_pred, dim = 1)\n        \n        tp = (y_true * y_pred).sum(dim = 0).to(torch.float32)\n        tn = ((1 - y_true) * (1 - y_pred)).sum(dim = 0).to(torch.float32)\n        fp = ((1 - y_true) * y_pred).sum(dim = 0).to(torch.float32)\n        fn = (y_true * (1 - y_pred)).sum(dim = 0).to(torch.float32)\n\n        precision = tp / (tp + fp + self.epsilon)\n        recall = tp / (tp + fn + self.epsilon)\n\n        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n        f1 = f1.clamp(min = self.epsilon, max = 1 - self.epsilon)\n        return 1 - f1.mean()\n\nF1_loss_function = F1_Loss().cuda()\n\nuse_f1_loss = False\nif use_f1_loss:\n    loss_function = F1_loss_function\nelse:\n    loss_function = XE_loss_function","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.795672Z","iopub.execute_input":"2023-09-08T14:27:09.796014Z","iopub.status.idle":"2023-09-08T14:27:09.807418Z","shell.execute_reply.started":"2023-09-08T14:27:09.795981Z","shell.execute_reply":"2023-09-08T14:27:09.806390Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"add_extra_embeds = [\n    'cnn_128',\n    #'sentbert',\n    #'laser_ta',\n    #'laser_en',\n]\n\nembeds_files = {\n    'cnn_128': ['/kaggle/input/embeddings/cnn_emb_train_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_test_128_kannada.npy'],\n     'laser_kn': ['/kaggle/input/embeddings/cnn_emb_train_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_test_128_kannada.npy'],\n     'laser_en': ['/kaggle/input/embeddings/cnn_emb_train_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_test_128_kannada.npy'],\n     'sentbert': ['/kaggle/input/embeddings/cnn_emb_train_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy']\n}\n\ntrain_embeddings = [np.load(embeds_files[embname][0]) for embname in add_extra_embeds]\ndev_embeddings = [np.load(embeds_files[embname][1]) for embname in add_extra_embeds]\ntest_embeddings = [np.load(embeds_files[embname][2]) for embname in add_extra_embeds]\ntorch.Tensor(train_embeddings).shape","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.808798Z","iopub.execute_input":"2023-09-08T14:27:09.809158Z","iopub.status.idle":"2023-09-08T14:27:10.031481Z","shell.execute_reply.started":"2023-09-08T14:27:09.809124Z","shell.execute_reply":"2023-09-08T14:27:10.030452Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 4695, 128])"},"metadata":{}}]},{"cell_type":"code","source":"len_extra_embeds = len(add_extra_embeds)\nprint(len_extra_embeds)\n\ndim_extra_embeds = np.sum([train_embeddings[i].shape[1] for i in range(len_extra_embeds)])\nprint(dim_extra_embeds)\n\nprint(lin_dim + dim_extra_embeds)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:10.033014Z","iopub.execute_input":"2023-09-08T14:27:10.033449Z","iopub.status.idle":"2023-09-08T14:27:10.039997Z","shell.execute_reply.started":"2023-09-08T14:27:10.033414Z","shell.execute_reply":"2023-09-08T14:27:10.038773Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1\n128\n3200\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import classification_report, f1_score\n\nfusion_classifier = FusionNet(int(3200), int(3200), 1024, 256, 64, 5)\n# Optimiser\noptimizer = AdamW(fusion_classifier.parameters(), lr = 0.0001)\nmo,ex_em = \"\",\"\"\nfor mod in model_names:\n    mo += mod\nmodel_name = 'fusion_kannada_'+ mo\n\ndevice = torch.device('cuda')\nfusion_classifier.to(device)\nfor model in models:\n    model.to(device)\nbest_val_f1 = 0\ncount = 0\n\n# Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size = 10, shuffle = True)\ndev_loader = DataLoader(dev_dataset, batch_size = 10, shuffle = False)\ntest_loader = DataLoader(test_dataset, batch_size = 10, shuffle = False)\nPATH = \"\"\nfor epoch in range(100):\n    fusion_classifier.train()\n    print(\"==========================================================\")\n    print(\"Epoch {}\".format(epoch))\n    print(\"Train\")\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        outputs_all = []\n        for i in range(n_models):\n            model = models[i]\n            input_ids = batch['input_ids' + '_' + str(i)].to(device)\n            attention_mask = batch['attention_mask' + '_' + str(i)].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask = attention_mask)\n            outputs_all.append(outputs[1])\n        \n        for i in range(len_extra_embeds):\n            print(torch.Tensor(train_embeddings[i].shape))\n            print(batch['index'])\n            outputs_all.append(torch.Tensor(train_embeddings[i][batch['index'], :]).to(device))\n            \n        bert_output = torch.cat(outputs_all, dim = -1) \n        out = fusion_classifier(bert_output)\n        loss = loss_function(out, labels)\n        loss.backward()\n        optimizer.step()\n    \n    print(\"Dev\")\n    dev_preds,dev_probs = [],[]\n    fusion_classifier.eval()\n    total_val_loss = 0\n    with torch.set_grad_enabled(False):\n        for batch in tqdm(dev_loader):\n            outputs_all = []\n            for i in range(n_models):\n                model = models[i]\n                input_ids = batch['input_ids' + '_' + str(i)].to(device)\n                attention_mask = batch['attention_mask' + '_' + str(i)].to(device)\n                labels = batch['labels'].to(device)\n                outputs = model(input_ids, attention_mask = attention_mask)\n                outputs_all.append(outputs[1])\n                \n            for i in range(len_extra_embeds):\n                outputs_all.append(torch.Tensor(dev_embeddings[i][batch['index'], :]).to(device))\n\n            bert_output = torch.cat(outputs_all, dim = -1) \n            out = fusion_classifier(bert_output)\n            loss = loss_function(out, labels)\n            total_val_loss += loss.item()/len(dev_loader)\n            \n            for logits in out.cpu().numpy():\n                dev_preds.append(np.argmax(logits))\n            for logits in out.cpu().numpy():\n                dev_probs.append(np.exp(logits)/np.sum(np.exp(logits)))\n    \n    y_true = dev_batch_labels\n    y_pred = dev_preds\n    target_names = label_mapping.keys()\n    report = classification_report(y_true, y_pred, target_names=target_names)\n    val_f1 = f1_score(y_true, y_pred, average = 'macro')\n    \n    if val_f1 > best_val_f1:\n        PATH = '/kaggle/working/'\n#         torch.save(fusion_classifier.state_dict())\n        best_val_f2 = val_f1\n        \n        #save dev preds\n#         with open('/kaggle/working/', 'wb') as f:\n#             print(np.array(dev_probs).shape)\n#             np.save(f + str(model_name) + '.npy', np.array(dev_probs))\n            \n        #find test preds\n        print(\"Test\")\n        test_preds,test_probs = [],[]\n        fusion_classifier.eval()\n        total_test_loss = 0\n        with torch.set_grad_enabled(False):\n            for batch in tqdm(test_loader):\n                outputs_all = []\n                for i in range(n_models):\n                    model = models[i]\n                    input_ids = batch['input_ids' + '_' + str(i)].to(device)\n                    attention_mask = batch['attention_mask' + '_' +str(i)].to(device)\n                    labels = batch['labels'].to(device)\n                    outputs = model(input_ids, attention_mask=attention_mask)\n                    outputs_all.append(outputs[1])\n\n                for i in range(len_extra_embeds):\n                    outputs_all.append(torch.Tensor(test_embeddings[i][batch['index'], :]).to(device))\n\n                bert_output = torch.cat(outputs_all, dim = -1) \n                out = fusion_classifier(bert_output)\n                loss = loss_function(out, labels)\n                total_test_loss += loss.item()/len(test_loader)\n\n                for logits in out.cpu().numpy():\n                    test_preds.append(np.argmax(logits))\n                for logits in out.cpu().numpy():\n                    test_probs.append(np.exp(logits)/np.sum(np.exp(logits)))\n\n        y_pred = test_preds\n        target_names = label_mapping.keys()\n#         with open('/kaggle/working/', 'wb') as f:\n#             print(np.array(test_probs).shape)\n#             np.save(f + str(model_name)+'.npy', np.array(test_probs))\n            \n        best_val_f1 = val_f1\n        count = 0\n    else:\n        count += 1\n    \n    print(report)\n    print(\"Epoch {}, Val Loss = {}, Val F1 = {}, Best Val f1 = {}, stagnant_t = {}\".format(epoch, total_val_loss, val_f1, best_val_f1, count))\n    if count == 5:\n        print(\"No increase for 5 epochs, Stopping ...\")\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:19.999597Z","iopub.status.idle":"2023-09-08T14:27:20.000078Z","shell.execute_reply.started":"2023-09-08T14:27:19.999834Z","shell.execute_reply":"2023-09-08T14:27:19.999858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import classification_report, f1_score\n\ndevice = torch.device('cuda')\nmodel.to(device)\n\n# Dataloaders\ndev_loader = DataLoader(dev_dataset, batch_size = 16, shuffle = False)\n# fusion_classifier.load_state_dict(torch.load('/kaggle/working/' + str(model_name) + '.pth'))\nfusion_classifier.eval()\n\ndev_preds = []\nwith torch.set_grad_enabled(False):\n    for batch in tqdm(dev_loader):\n        outputs_all = []\n        for i in range(n_models):\n            model = models[i]\n            input_ids = batch['input_ids'+'_'+str(i)].to(device)\n            attention_mask = batch['attention_mask'+'_'+str(i)].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            outputs_all.append(outputs[1])\n            \n        for i in range(len_extra_embeds):\n            outputs_all.append(torch.Tensor(dev_embeddings[i][batch['index'], :]).to(device))\n\n        bert_output = torch.cat(outputs_all, dim = -1) \n        out = fusion_classifier(bert_output)\n            \n        for logits in out.cpu().numpy():\n            dev_preds.append(np.argmax(logits))","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:20.002173Z","iopub.status.idle":"2023-09-08T14:27:20.002691Z","shell.execute_reply.started":"2023-09-08T14:27:20.002433Z","shell.execute_reply":"2023-09-08T14:27:20.002457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = dev_batch_labels\ny_pred = dev_preds\ntarget_names = label_mapping.keys()\nreport = classification_report(y_true, y_pred, target_names = target_names)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:20.004404Z","iopub.status.idle":"2023-09-08T14:27:20.004875Z","shell.execute_reply.started":"2023-09-08T14:27:20.004637Z","shell.execute_reply":"2023-09-08T14:27:20.004660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}