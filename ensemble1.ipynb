{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6076119,"sourceType":"datasetVersion","datasetId":3478107},{"sourceId":6228507,"sourceType":"datasetVersion","datasetId":3577624},{"sourceId":6402537,"sourceType":"datasetVersion","datasetId":3691582}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:22:41.000762Z","iopub.execute_input":"2023-09-08T14:22:41.001109Z","iopub.status.idle":"2023-09-08T14:22:41.012996Z","shell.execute_reply.started":"2023-09-08T14:22:41.001078Z","shell.execute_reply":"2023-09-08T14:22:41.011895Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\n!pip install langdetect\nfrom langdetect import detect\nwarnings.filterwarnings(\"ignore\")\n!pip install clean-text\nfrom cleantext import clean\n","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:22:41.015006Z","iopub.execute_input":"2023-09-08T14:22:41.015936Z","iopub.status.idle":"2023-09-08T14:23:16.352134Z","shell.execute_reply.started":"2023-09-08T14:22:41.015902Z","shell.execute_reply":"2023-09-08T14:23:16.350989Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=8e512132d3b8378bba5865cf63cfafa977d0ac68ebe91620138df33a75d1e4eb\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\nCollecting clean-text\n  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\nCollecting emoji<2.0.0,>=1.0.0 (from clean-text)\n  Downloading emoji-1.7.0.tar.gz (175 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy<7.0,>=6.0 (from clean-text)\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\nBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=e817b1e99d871438f686ba7fa89d53826fbd7344866c6bd8b746c979e6e8d7f7\n  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\nSuccessfully built emoji\nInstalling collected packages: emoji, ftfy, clean-text\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.6.0\n    Uninstalling emoji-2.6.0:\n      Successfully uninstalled emoji-2.6.0\nSuccessfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"def read(url):\n    df = pd.read_csv(url)\n    return df\ntrain = read('/kaggle/input/augmentedk/augmented_dataset/train_oversample.csv')\nval = read('/kaggle/input/offensivek/Datasets/Trans_kan_dev.csv')\ntest = read('/kaggle/input/offensivek/Datasets/Trans_kan_test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:23:16.354396Z","iopub.execute_input":"2023-09-08T14:23:16.354770Z","iopub.status.idle":"2023-09-08T14:23:16.570417Z","shell.execute_reply.started":"2023-09-08T14:23:16.354734Z","shell.execute_reply":"2023-09-08T14:23:16.569442Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nval = val.append(test)\ndef remnotk(offense):\n    if (offense == 'not-Kannada' or offense == 'label' or offense == 5):\n        return None\n    else:\n        return offense\ntrain['labels'] = train['labels'].apply(remnotk)\nval['label'] = val['label'].apply(remnotk)\n\n\nimport re\ndef clean_text(text):\n    text = re.sub(r\"@[^A-Za-z0-9]+\", '', text)\n    text = re.sub(r\"@[A-Za-z0-9]+\", '', text)\n    text = re.sub(r\"https?://[A-Za-z0-9./]+\", '', text)\n    #text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n    text = re.sub('\\t', '',  text)\n    text = re.sub(r\" +\", '', text)\n    return text    \ntrain['tweets'] = train['tweets'].apply(clean_text)\nval['transliterated'] = val['transliterated'].apply(clean_text)\n\n\ndef cleannotk(transliterated):\n    detected_language = detect(transliterated)\n    if (detected_language != 'kn'):\n        print(detected_language, transliterated)\n        return None\n    else:\n        return transliterated\nval['transliterated'] = val['transliterated'].apply(cleannotk)\ntrain['tweets'] = train['tweets'].apply(cleannotk)\n\n\ntrain = train.dropna()\ntrain = train.reset_index()\nprint(train)\nval = val.dropna()\nval = val.reset_index()\nprint(train)\n\ntrain['labels'] = train['labels']\ntrain['tweets'] = train['tweets']\n\nfor i in range(len(train)):\n    train['tweets'][i] = train['tweets'][i][8:-2]\ntrain = train.drop(columns = ['Unnamed: 0'])\nprint('Training dataset:\\n', train)\n\nval['labels'] = LabelEncoder().fit_transform( val['label'] )\nval['tweets'] = val['transliterated']\n\nfor i in range(len(val)):\n    val['tweets'][i] = val['tweets'][i][8:-2]\nval = val.drop(columns = ['label', 'Unnamed: 0', 'transliterated', 'Sentence'])\nprint('Training dataset:\\n', val)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:23:16.571696Z","iopub.execute_input":"2023-09-08T14:23:16.572050Z","iopub.status.idle":"2023-09-08T14:24:20.349282Z","shell.execute_reply.started":"2023-09-08T14:23:16.572002Z","shell.execute_reply":"2023-09-08T14:24:20.348235Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"sl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'2'}\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'1*50'}\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nhr {'kn':'u200dü§¶\\u200d'}\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nml {'kn':'‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡µÄ‡¥∏‡µç‡¥π‡¥æ‡µª‡¥°‡µç\\u200c‡¥∏‡µç‡¥Ö‡¥™‡µç‡¥™‡µç\\u200c'}\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nta {'kn':'‡ÆÆ‡Æø‡Æï‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ©‡Æ™‡Ææ‡Æü‡Æ≤‡Øç.‡Æá‡Æö‡Øà‡ÆØ‡ØÅ‡ÆÆ‡Øç‡ÆÆ‡Æø‡Æï‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà.‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç‡Æá‡Æ∞‡Æï‡Øç‡Æ∑‡Øç‡Æ§‡Øç‡Æ∑‡ØÜ‡Æü‡Øç‡Æü‡Æø.'}\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nml {'kn':'‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥ï‡µæ‡¥â‡¥£‡µç‡¥ü‡µá‡µΩ‡¥≤‡µà‡¥ï‡µç\\u200c‡¥Ö‡¥ü‡¥ø‡¥ö‡µç‡¥ö‡¥ø‡¥ü‡µç‡¥ü‡µç‡¥™‡µã‡¥ü‡µÜ'}\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'46449449494'}\nsl {'kn':'24'}\nfi {'kn':'\nhi {'kn':'‡§≠‡§æ‡§à‡§π‡§ø‡§Ç‡§¶‡•Ä‡§Æ‡•á‡§Ç‡§ï‡§¨‡§Ü‡§Ø‡•á‡§ó‡•Ä'}\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nhr {'kn':'‡≤®‡≤®‡≥ç‡≤®‡≤æ‡≤ó‡≤§‡≥Å‡≤¨‡≥ç‡≤¨‡≤¨‡≤ú‡≤æ‡≤∞‡≥Å‡≤∏‡≤æ‡≤∞‡≥ç\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d\\u200d'}\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nfi {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nsl {'kn':'\nfi {'kn':'\nhi ‡≤Æ‡≤∞‡≤°‡≥ç‡≤°‡≤ø‡§π‡§ø‡§®‡•ç‡§¶‡•Ä‡§ë‡§°‡§ø‡§è‡§Ç‡§∏‡§Æ‡•á‡§Ç‡§≠‡•Ä‡≤Ü‡≤®‡≥ç‡≤∏‡≥ç‡§ï‡§æ‡§ó‡§ú‡§¨‡§ï‡•ç‡§∞‡•á‡§ú‡§π‡•à‡•§‡§ú‡§∞‡•Ç‡§∞‡§¶‡•á‡§ñ‡•á‡§Ç‡§ó‡•á‡§á‡§∏‡§´‡§ø‡§≤‡•ç‡§Æ‡§ï‡•ã‡•§‡≤µ‡≥á‡≤µ‡≤ø‡≤≤‡≥ç‡≤°‡≤ø‡≤´‡≤ø‡≤®‡≤ø‡≤ü‡≥Ü‡≤≤‡≤ø‡≤µ‡≤æ‡≤ö‡≥ç‡≤¶‡≤ø‡≤∏‡≥ç‡≤Æ‡≥Ç‡≤µ‡≤ø....\nta ‡≤Æ‡≤∏‡≥ç‡≤ü‡≥ç‡≤µ‡≤æ‡≤ö‡≥ç‡≤Æ‡≤¶‡≤∞‡≥ç‡≤∏‡≥Ü‡≤Ç‡≤ü‡≤ø‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç‡≤µ‡≥Ä‡≤°‡≤ø‡≤Ø‡≥ã..‡≤®‡≥ç‡Æ§‡Ææ‡ÆØ‡Æø‡Æ©‡Øç‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡Æ§‡ØÜ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç‡Æá‡Æ§‡Øà‡Æ§‡Æµ‡Æ±‡Æµ‡Æø‡Æü‡Ææ‡Æ§‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç.‡≤®‡≥Ä‡≤®‡≥ç\nml ‡¥Ö‡¥ü‡¥ø‡¥™‡µä‡¥≥‡¥ø‡¥™‡¥ü‡¥Ç..‡¥®‡¥æ‡¥Ø‡¥ø‡¥ï‡¥ï‡µÅ‡¥ü‡µç‡¥ü‡µÇ‡¥∏‡µç‡¥™‡µä‡¥≥‡¥ø\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nhi ‡≤∞‡≥Ü‡≤ï‡≥ç‡≤∏‡≥ç‡§ï‡§¨‡§§‡§ï‡§π‡•ã‡§∏‡§ï‡§§‡•Ä‡§π‡•à‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º\nte ‡∞ú‡±à‡∞°‡∞ø‡∞¨‡∞æ‡∞∏‡±ç‡∞Ö‡∞≠‡∞ø‡∞Æ‡∞æ‡∞®‡∞ø\nhi ‡§Ö‡§∞‡•á‡§µ‡§æ‡§π...‡§ó‡§ú‡§¨‡§ï‡•Ä‡§´‡•Ä‡§≤‡§π‡•à‡§á‡§∏‡§ó‡§æ‡§®‡•á‡§Æ‡•á‡§Ç..?‡≤´‡≥ç‡≤Ø‡≤æ‡≤®‡≥ç‡≤´‡≥ç‡≤∞‡≤Æ‡≥ç‡≤ó‡≥Å‡≤ú‡≤∞‡≤æ‡≤§‡≥ç\nml ‡≤∏‡≥Ç‡≤™‡≤∞‡≥ç....‡≤ï‡≥á‡≤∞‡≤≥‡≤∞‡≤ï‡≥ç‡≤∑‡≤ø‡≤§‡≥ç‡≤∂‡≥Ü‡≤ü‡≥ç‡≤ü‡≤ø‡≤´‡≥ç‡≤Ø‡≤æ‡≤®‡≥ç‡≤∏‡≥ç‡≤≤‡≥à‡≤ï‡≥ç‡≤Æ‡≤æ‡≤°‡≤ø.‡¥≤‡µà‡¥ï‡µç‡¥ï‡µç‡¥Ö‡¥ü‡¥ø‡¥ï‡µç‡¥ï‡µÇ‡¥∏‡µÅ‡¥π‡µÉ‡¥§‡µç‡¥§‡µÅ‡¥ï‡µç‡¥ï‡¥≥‡µÜ...\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nfr ‡≤π‡≤Ç‡≤¶‡≤Ö‡≤™‡≥ç\\u200d\\u200d\\u200d\nml ‡¥∏‡µÇ‡¥™‡µç‡¥™‡µº‡¥Æ‡µÇ‡¥µ‡¥ø\nbg —ÉœÉœÖ—èŒΩŒπ‚àÇ—îœÉŒπ—ï—èŒπ‡≤ú‡≤ø‡≤é–Ω—Ç–≤—èœÉ\nml ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡µÄ‡¥∏‡µç‡¥â‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ‡≤≤‡≥à‡≤ï‡≥ç‡≤¨‡≤ü‡≥ç‡≤ü‡≤®‡≥ç‡≤Ö‡≤°‡≤ø‡≤ö‡≥ç‡¥™‡µã‡¥ü‡µç‡¥ü‡¥ø‡¥ï‡¥ü‡≤Æ‡≤ï‡≥ç‡≤ï‡≤≥‡≥Ü..\nhi ‡§∏‡§∞‡§Ü‡§™‡§≤‡•ã‡§ó‡•ã‡§∏‡•á‡§®‡§ø‡§µ‡•á‡§¶‡§®‡§π‡•à‡§Ü‡§™‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡•ù‡§Æ‡§ß‡•ç‡§Ø‡•ç‡§™‡•ç‡§∞‡§¶‡•á‡§∂‡§á‡§∏‡§§‡§∞‡•û‡§ñ‡•Å‡§¶‡§ï‡•Ä‡§ü‡§æ‡§ï‡•Ä‡§ú‡§ñ‡§∞‡•Ä‡§¶‡§≤‡•Ä‡§ú‡§ø‡§Ø‡•á‡§ú‡§ø‡§∏‡§Æ‡•á‡§∏‡§ø‡§∞‡•ç‡§´‡§∏‡§æ‡§â‡§•‡§ï‡•Ä‡§°‡§¨‡§Æ‡•Ç‡§µ‡•Ä‡§π‡•Ä‡§ö‡§≤‡•á‡§ó‡•Ä‡§Ü‡§™‡§∏‡•á‡§∞‡§ø‡§ï‡•ç‡§µ‡•á‡§∏‡•ç‡§ü‡§π‡•à‡§Ü‡§™‡§™‡•à‡§∏‡§æ‡§á‡§®‡•ç‡§µ‡•á‡§∏‡•ç‡§ü‡§ï‡§∞‡§ï‡•á‡§¶‡§ø‡§ñ‡•á‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡•ù‡§¶‡•Å‡§∞‡•ç‡§ó‡§Æ‡•á‡§Ç‡§§‡§∞‡•Å‡§£‡§¨‡§ø‡§ó‡§∏‡§ø‡§®‡•á‡§Æ‡§æ‡§ñ‡§æ‡§≤‡•Ä‡§π‡•ã‡§∞‡§π‡§æ‡§π‡•à‡§Ü‡§™‡§¶‡•Å‡§∞‡•ç‡§ó‡§Æ‡•á‡§Ç‡§á‡§®‡•ç‡§µ‡•á‡§∏‡•ç‡§ü‡§ï‡§∞‡§∏‡§ï‡§§‡•á‡§π‡•à‡≤®‡≥ç‡§´‡§ø‡§∞‡§ï‡•ç‡§Ø‡§æ‡§Ü‡§™‡§≤‡•ã‡§ó‡§ß‡•Ä‡§∞‡•á‡§ß‡•Ä‡§∞‡•á‡§™‡•Ç‡§∞‡•á‡§π‡§∞‡§ú‡§ø‡§≤‡•ã‡§Ç‡§Æ‡•á‡§Ç1‡§∏‡§ø‡§®‡•á‡§Æ‡§æ‡§ò‡§∞‡§Ü‡§™‡§Ö‡§≤‡•â‡§ü‡§ï‡§∞‡§§‡•á‡§ú‡§æ‡§®‡§æ‡§´‡§ø‡§∞‡§¶‡•á‡§ñ‡§®‡§æ‡§Ü‡§™‡§ï‡•Ä‡§π‡§∞‡§Æ‡•Ç‡§µ‡•Ä‡§ï‡•ã‡§π‡§Æ‡§¶‡•á‡§ñ‡§∏‡§ï‡§§‡•á‡§π‡•à‡§î‡§∞‡§Ö‡§ö‡•ç‡§õ‡§æ‡§¨‡§ø‡§ú‡§®‡•á‡§∏‡§≠‡•Ä‡§ï‡§∞‡§∏‡§ï‡•á‡§ó‡•Ä\nta ‡≤Æ‡≤∏‡≥ç‡≤ü‡≥ç‡≤µ‡≤æ‡≤ö‡≥ç‡≤Æ‡≤¶‡≤∞‡≥ç‡≤∏‡≥Ü‡≤Ç‡≤ü‡≤ø‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç‡≤µ‡≥Ä‡≤°‡≤ø‡≤Ø‡≥ã..‡≤®‡≥ç‡Æ§‡Ææ‡ÆØ‡Æø‡Æ©‡Øç‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡Æ§‡ØÜ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç‡Æá‡Æ§‡Øà‡Æ§‡Æµ‡Æ±‡Æµ‡Æø‡Æü‡Ææ‡Æ§‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç.‡≤®‡≥Ä‡≤®‡≥ç\nhi ‡≤∞‡≥Ü‡≤ï‡≥ç‡≤∏‡≥ç‡§ï‡§¨‡§§‡§ï‡§π‡•ã‡§∏‡§ï‡§§‡•Ä‡§π‡•à‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º\nhi ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä‡§Æ‡•á‡§Ç‡§ï‡§¨‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º‡§π‡•ã‡§ó‡•Ä‡§á‡§Ç‡§§‡§ú‡§æ‡§∞‡§ï‡§∞‡§∞‡§π‡•á‡§π‡•à‡§Ç\nhi ‡§∏‡§∞‡§Ü‡§™‡§≤‡•ã‡§ó‡•ã‡§∏‡•á‡§®‡§ø‡§µ‡•á‡§¶‡§®‡§π‡•à‡§Ü‡§™‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡•ù‡§Æ‡§ß‡•ç‡§Ø‡•ç‡§™‡•ç‡§∞‡§¶‡•á‡§∂‡§á‡§∏‡§§‡§∞‡•û‡§ñ‡•Å‡§¶‡§ï‡•Ä‡§ü‡§æ‡§ï‡•Ä‡§ú‡§ñ‡§∞‡•Ä‡§¶‡§≤‡•Ä‡§ú‡§ø‡§Ø‡•á‡§ú‡§ø‡§∏‡§Æ‡•á‡§∏‡§ø‡§∞‡•ç‡§´‡§∏‡§æ‡§â‡§•‡§ï‡•Ä‡§°‡§¨‡§Æ‡•Ç‡§µ‡•Ä‡§π‡•Ä‡§ö‡§≤‡•á‡§ó‡•Ä‡§Ü‡§™‡§∏‡•á‡§∞‡§ø‡§ï‡•ç‡§µ‡•á‡§∏‡•ç‡§ü‡§π‡•à‡§Ü‡§™‡§™‡•à‡§∏‡§æ‡§á‡§®‡•ç‡§µ‡•á‡§∏‡•ç‡§ü‡§ï‡§∞‡§ï‡•á‡§¶‡§ø‡§ñ‡•á‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡•ù‡§¶‡•Å‡§∞‡•ç‡§ó‡§Æ‡•á‡§Ç‡§§‡§∞‡•Å‡§£‡§¨‡§ø‡§ó‡§∏‡§ø‡§®‡•á‡§Æ‡§æ‡§ñ‡§æ‡§≤‡•Ä‡§π‡•ã‡§∞‡§π‡§æ‡§π‡•à‡§Ü‡§™‡§¶‡•Å‡§∞‡•ç‡§ó‡§Æ‡•á‡§Ç‡§á‡§®‡•ç‡§µ‡•á‡§∏‡•ç‡§ü‡§ï‡§∞‡§∏‡§ï‡§§‡•á‡§π‡•à‡≤®‡≥ç‡§´‡§ø‡§∞‡§ï‡•ç‡§Ø‡§æ‡§Ü‡§™‡§≤‡•ã‡§ó‡§ß‡•Ä‡§∞‡•á‡§ß‡•Ä‡§∞‡•á‡§™‡•Ç‡§∞‡•á‡§π‡§∞‡§ú‡§ø‡§≤‡•ã‡§Ç‡§Æ‡•á‡§Ç1‡§∏‡§ø‡§®‡•á‡§Æ‡§æ‡§ò‡§∞‡§Ü‡§™‡§Ö‡§≤‡•â‡§ü‡§ï‡§∞‡§§‡•á‡§ú‡§æ‡§®‡§æ‡§´‡§ø‡§∞‡§¶‡•á‡§ñ‡§®‡§æ‡§Ü‡§™‡§ï‡•Ä‡§π‡§∞‡§Æ‡•Ç‡§µ‡•Ä‡§ï‡•ã‡§π‡§Æ‡§¶‡•á‡§ñ‡§∏‡§ï‡§§‡•á‡§π‡•à‡§î‡§∞‡§Ö‡§ö‡•ç‡§õ‡§æ‡§¨‡§ø‡§ú‡§®‡•á‡§∏‡§≠‡•Ä‡§ï‡§∞‡§∏‡§ï‡•á‡§ó‡•Ä\nml ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡µÄ‡¥∏‡µç‡¥â‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ‡≤≤‡≥à‡≤ï‡≥ç‡≤¨‡≤ü‡≥ç‡≤ü‡≤®‡≥ç‡≤Ö‡≤°‡≤ø‡≤ö‡≥ç‡¥™‡µã‡¥ü‡µç‡¥ü‡¥ø‡¥ï‡¥ü‡≤Æ‡≤ï‡≥ç‡≤ï‡≤≥‡≥Ü..\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nta ‡Æ§‡Æø‡ÆØ‡Ææ‡Æí‡Æ∞‡ØÅ‡ÆÖ‡Æ¥‡Æï‡Ææ‡Æ©‡Æï‡Ææ‡Æ§‡Æ≤‡Øç‡Æï‡Ææ‡Æµ‡Æø‡ÆØ‡ÆÆ‡Øç.‡ÆÖ‡Æ©‡Øà‡Æ§‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øä‡Æ¥‡Æø‡ÆØ‡Æø‡Æ≤‡ØÅ‡ÆÆ‡Øç‡Æì‡Æ∞‡Øç‡Æ§‡Ææ‡Æï‡Øç‡Æï‡Æ§‡Øç‡Æ§‡Øà‡Æè‡Æ±‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æø‡ÆØ‡Æ§‡Æø‡Æ∞‡Øà‡Æ™‡Øç‡Æ™‡Æü‡ÆÆ‡Øç.‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç‡Æö‡Ææ‡Æ∞‡Øç‡Æ™‡Ææ‡Æï‡Æé‡Æ©‡Æ§‡ØÅ‡ÆÆ‡Æ©‡ÆÆ‡Ææ‡Æ∞‡Øç‡Æ®‡Øç‡Æ§‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç.\nml ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥ø‡¥ï‡µæ‡¥í‡¥£‡µç‡¥ü‡µã‡¥á‡¥µ‡¥ø‡¥ü‡µÜ\nfr ‡≤π‡≤Ç‡≤¶‡≤Ö‡≤™‡≥ç\\u200d\\u200d\\u200d\nml ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡µÄ‡¥∏‡µç‡¥Ü‡¥∞‡µÅ‡¥Ç‡¥á‡¥≤‡µç‡¥≤‡µá.‡¥à‡¥™‡¥ü‡¥Ç‡¥á‡¥§‡µÅ‡¥µ‡¥∞‡µÜ‡¥ï‡¥æ‡¥£‡¥æ‡¥§‡µç‡¥§‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥ø‡¥ï‡µæ‡¥≤‡µà‡¥ï‡µç\nml ‡¥ï‡¥ü‡µç‡¥ü‡¥µ‡µÜ‡¥Ø‡µç‡¥±‡µç‡¥±‡¥ø‡¥ô‡µç‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç‡≤Ü‡≤®‡≥ç‡≤∏‡≥ç\nml ‡¥∏‡µÇ‡¥™‡µç‡¥™‡µº‡¥Æ‡µÇ‡¥µ‡¥ø\nta ‡Æö‡ØÇ‡Æ™‡Øç‡Æ™‡Æ∞‡Øç‡Æö‡Ææ‡Æô‡Øç‡Æï‡Øç‡ÆÖ‡Æ£‡Øç‡Æü‡ØÅ‡Æµ‡Æø‡Æö‡ØÅ‡Æµ‡Æ≤‡Øç‡Æé‡Æ™‡Æï‡Øç‡Æü‡Øç\nta ‡≤Æ‡≤∏‡≥ç‡≤ü‡≥ç‡≤µ‡≤æ‡≤ö‡≥ç‡≤Æ‡≤¶‡≤∞‡≥ç‡≤∏‡≥Ü‡≤Ç‡≤ü‡≤ø‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç‡≤µ‡≥Ä‡≤°‡≤ø‡≤Ø‡≥ã..‡≤®‡≥ç‡Æ§‡Ææ‡ÆØ‡Æø‡Æ©‡Øç‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡Æ§‡ØÜ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç‡Æá‡Æ§‡Øà‡Æ§‡Æµ‡Æ±‡Æµ‡Æø‡Æü‡Ææ‡Æ§‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç.‡≤®‡≥Ä‡≤®‡≥ç\nhi ‡§Ö‡§∞‡•á‡§µ‡§æ‡§π...‡§ó‡§ú‡§¨‡§ï‡•Ä‡§´‡•Ä‡§≤‡§π‡•à‡§á‡§∏‡§ó‡§æ‡§®‡•á‡§Æ‡•á‡§Ç..?‡≤´‡≥ç‡≤Ø‡≤æ‡≤®‡≥ç‡≤´‡≥ç‡≤∞‡≤Æ‡≥ç‡≤ó‡≥Å‡≤ú‡≤∞‡≤æ‡≤§‡≥ç\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nml ‡¥∏‡µÇ‡¥™‡µç‡¥™‡µº‡¥Æ‡µÇ‡¥µ‡¥ø\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nml ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥ø‡¥ï‡µæ‡¥í‡¥£‡µç‡¥ü‡µã‡¥á‡¥µ‡¥ø‡¥ü‡µÜ\nml ‡¥∞‡¥ï‡µç‡¥∑‡¥ø‡¥§‡µç‡¥∑‡µÜ‡¥ü‡µç‡¥ü‡¥ø‡¥ï‡µá‡¥∞‡¥≥‡¥´‡¥æ‡µª‡¥∏‡µç‡≤®‡≥ç‡≤∞‡≤æ‡≤ï‡≥ç‡≤∑‡≤ø‡≤§‡≥ç‡≤∂‡≥Ü‡≤ü‡≥ç‡≤ü‡≤ø‡≤ï‡≥á‡≤∞‡≤≥‡≤´‡≥ç‡≤Ø‡≤æ‡≤®‡≥ç‡≤∏‡≥ç\nml ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡µÄ‡¥∏‡µç‡¥â‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ‡≤≤‡≥à‡≤ï‡≥ç‡≤¨‡≤ü‡≥ç‡≤ü‡≤®‡≥ç‡≤Ö‡≤°‡≤ø‡≤ö‡≥ç‡¥™‡µã‡¥ü‡µç‡¥ü‡¥ø‡¥ï‡¥ü‡≤Æ‡≤ï‡≥ç‡≤ï‡≤≥‡≥Ü..\nml ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡µÄ‡¥∏‡µç‡¥â‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ‡≤≤‡≥à‡≤ï‡≥ç‡≤¨‡≤ü‡≥ç‡≤ü‡≤®‡≥ç‡≤Ö‡≤°‡≤ø‡≤ö‡≥ç‡¥™‡µã‡¥ü‡µç‡¥ü‡¥ø‡¥ï‡¥ü‡≤Æ‡≤ï‡≥ç‡≤ï‡≤≥‡≥Ü..\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nml ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡µÄ‡¥∏‡µç‡¥â‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ‡≤≤‡≥à‡≤ï‡≥ç‡≤¨‡≤ü‡≥ç‡≤ü‡≤®‡≥ç‡≤Ö‡≤°‡≤ø‡≤ö‡≥ç‡¥™‡µã‡¥ü‡µç‡¥ü‡¥ø‡¥ï‡¥ü‡≤Æ‡≤ï‡≥ç‡≤ï‡≤≥‡≥Ü..\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nmr ‡§ä‡§Ü‡§π‡•á‡§ï‡•Ä‡§®‡§æ‡§π‡•Ä‡≤®‡≥ç............‡≤®‡≥ç‡§π‡•á.....‡§π‡•á‡§π‡•Ä.‡§π‡§π‡§π\nta ‡≤Æ‡≤∏‡≥ç‡≤ü‡≥ç‡≤µ‡≤æ‡≤ö‡≥ç‡≤Æ‡≤¶‡≤∞‡≥ç‡≤∏‡≥Ü‡≤Ç‡≤ü‡≤ø‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç‡≤µ‡≥Ä‡≤°‡≤ø‡≤Ø‡≥ã..‡≤®‡≥ç‡Æ§‡Ææ‡ÆØ‡Æø‡Æ©‡Øç‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà‡Æ§‡ØÜ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç‡Æá‡Æ§‡Øà‡Æ§‡Æµ‡Æ±‡Æµ‡Æø‡Æü‡Ææ‡Æ§‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç.‡≤®‡≥Ä‡≤®‡≥ç\nhi ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä‡§Æ‡•á‡§Ç‡§ï‡§¨‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º‡§π‡•ã‡§ó‡•Ä‡§á‡§Ç‡§§‡§ú‡§æ‡§∞‡§ï‡§∞‡§∞‡§π‡•á‡§π‡•à‡§Ç\nml ‡¥ï‡¥ü‡µç‡¥ü‡¥µ‡µÜ‡¥Ø‡µç‡¥±‡µç‡¥±‡¥ø‡¥ô‡µç‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç‡≤Ü‡≤®‡≥ç‡≤∏‡≥ç\n       index  Unnamed: 0  labels  \\\n0          0           0     2.0   \n1          1           1     4.0   \n2          2           2     1.0   \n3          4           4     1.0   \n4          5           5     2.0   \n...      ...         ...     ...   \n24994  29994       29994     3.0   \n24995  29996       29996     4.0   \n24996  29997       29997     4.0   \n24997  29998       29998     3.0   \n24998  29999       29999     3.0   \n\n                                                  tweets  \n0             ‡≤®‡≥Ä‡≤µ‡≥Å‡≤ï‡≥ç‡≤≤‡≤ø‡≤Æ‡≤ï‡≥ç‡≤∏‡≥ç‡≤®‡≥ã‡≤°‡≤ø‡≤≤‡≥ç‡≤≤‡≤æ‡≤Ö‡≤¶‡≤ï‡≥ç‡≤ï‡≥Ü‡≤®‡≤ø‡≤Ç‡≤ó‡≥Ü‡≤á‡≤∑‡≥ç‡≤ü‡≤Ü‡≤ó‡≤ø‡≤≤‡≥ç‡≤≤  \n1                                          ‡≤á‡≤¶‡≥Å‡≤™‡≤ï‡≥ç‡≤ï‡≤∏‡≥Å‡≤≥‡≥ç‡≤≥‡≥Å  \n2           ‡≤ï‡≤°‡≥Å‡≤í‡≤°‡≤ø‡≤Ø‡≥Å‡≤µ‡≤Æ‡≤æ‡≤ú‡≤®‡≥Ü‡≤¨‡≥Ü‡≤∞‡≥Ü‡≤Æ‡≤ø‡≤®‡≥ç‡≤∏‡≤∞‡≥ç‡≤ü‡≥ç‡≤™‡≥Ä‡≤™‡≤≤‡≥ç‡≤ó‡≥Ü‡≤ü‡≥ç‡≤è‡≤Ü‡≤®‡≤∏‡≥ç‡≤µ‡≤∞‡≥ç  \n3      ‡≤¨‡≤∞‡≤ø‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ú‡≤®‡≤µ‡≥á‡≤¨‡≤∞‡≤¶‡≤´‡≤ø‡≤≤‡≥ç‡≤Æ‡≤∏‡≥ç‡≤®‡≥ã‡≤°‡≥ä‡≤¶‡≥Å‡≤´‡≥ç‡≤Ø‡≤æ‡≤∏‡≤ø‡≤ï‡≥ã‡≤∞‡≥ç‡≤Ø‡≥Å‡≤ü‡≥ç‡≤Ø‡≥Ç‡≤¨‡≥ç...  \n4                     ‡≤é‡≤≤‡≥ç‡≤≤‡≤®‡≥Ç‡≤ú‡≥Ü‡≤¨‡≥ç‡≤ó‡≥Ü‡≤Ø‡≤æ‡≤ï‡≥ç‡≤ï‡≤Ç‡≤™‡≥á‡≤∞‡≥ç‡≤Æ‡≤æ‡≤°‡≥ç‡≤§‡≤ø‡≤Ø‡≤æ‡≤ó‡≥Å‡≤∞‡≥Å  \n...                                                  ...  \n24994  ‡≤ü‡≥Ç‡≤Ü‡≤µ‡≤∞‡≥ç‡≤∏‡≥Ü‡≤Ç‡≤ü‡≤ø‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç‡≤®‡≤æ‡≤ü‡≤ö‡≥ç‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≤æ‡≤®‡≥ç‡≤®‡≥Ä‡≤∞‡≥ç‡≤π‡≤æ‡≤ï‡≥ç‡≤∏‡≤ø‡≤Ö‡≤¶‡≤®‡≤ï‡≤µ‡≤∞‡≥ç...  \n24995                           ‡≤∏‡≥Ç‡≤™‡≤∞‡≥ç‡≤Ö‡≤®‡≥ç‡≤®‡≤æ‡≤∏‡≤ï‡≤§‡≤æ‡≤ó‡≤ø‡≤â‡≤ó‡≥ç‡≤¶‡≤ø‡≤°‡≤ø‡≤∞  \n24996  ‡≤ï‡≤®‡≥ç‡≤®‡≤°‡≤¶‡≤≤‡≥ç‡≤≤‡≤ø‡≤Æ‡≤æ‡≤°‡≥ã‡≤¨‡≤¶‡≥ç‡≤≤‡≥Å‡≤π‡≤ø‡≤Ç‡≤¶‡≤ø‡≤®‡≤≤‡≥ç‡≤≤‡≤ø‡≤Æ‡≤æ‡≤°‡≤ø‡≤¶‡≥ç‡≤∞‡≥Ü‡≤∏‡≥Å‡≤™‡≤∞‡≥ç‡≤∞‡≤ø‡≤ü‡≥ç...  \n24997  ‡≤è‡≤ú‡≤®‡≤§‡≤æ‡≤∂‡≤§‡≥ç‡≤∞‡≥Å‡≤ï‡≥Ç‡≤°‡≤æ‡≤ü‡≤ø‡≤ï‡≥ç‡≤ü‡≤æ‡≤ï‡≥ç‡≤®‡≤æ‡≤Æ‡≤æ‡≤§‡≥ç‡≤∞‡≤æ‡≤Ø‡≥Ç‡≤®‡≤ø‡≤®‡≥ç‡≤∏‡≥ç‡≤ü‡≤≤‡≥ç‡≤Æ‡≤¶‡≥ã‡≤≥‡≤Ö...  \n24998                 ‡≤Æ‡≤æ‡≤∞‡≥ç‡≤ï‡≥Ü‡≤ü‡≤ø‡≤Ç‡≤ó‡≥ç‡≤∏‡≥ç‡≤µ‡≤≤‡≥ç‡≤™‡≤¨‡≥á‡≤ï‡≤ø‡≤§‡≥ç‡≤§‡≥Å‡≤Ö‡≤®‡≥ç‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≥Ü  \n\n[24999 rows x 4 columns]\n       index  Unnamed: 0  labels  \\\n0          0           0     2.0   \n1          1           1     4.0   \n2          2           2     1.0   \n3          4           4     1.0   \n4          5           5     2.0   \n...      ...         ...     ...   \n24994  29994       29994     3.0   \n24995  29996       29996     4.0   \n24996  29997       29997     4.0   \n24997  29998       29998     3.0   \n24998  29999       29999     3.0   \n\n                                                  tweets  \n0             ‡≤®‡≥Ä‡≤µ‡≥Å‡≤ï‡≥ç‡≤≤‡≤ø‡≤Æ‡≤ï‡≥ç‡≤∏‡≥ç‡≤®‡≥ã‡≤°‡≤ø‡≤≤‡≥ç‡≤≤‡≤æ‡≤Ö‡≤¶‡≤ï‡≥ç‡≤ï‡≥Ü‡≤®‡≤ø‡≤Ç‡≤ó‡≥Ü‡≤á‡≤∑‡≥ç‡≤ü‡≤Ü‡≤ó‡≤ø‡≤≤‡≥ç‡≤≤  \n1                                          ‡≤á‡≤¶‡≥Å‡≤™‡≤ï‡≥ç‡≤ï‡≤∏‡≥Å‡≤≥‡≥ç‡≤≥‡≥Å  \n2           ‡≤ï‡≤°‡≥Å‡≤í‡≤°‡≤ø‡≤Ø‡≥Å‡≤µ‡≤Æ‡≤æ‡≤ú‡≤®‡≥Ü‡≤¨‡≥Ü‡≤∞‡≥Ü‡≤Æ‡≤ø‡≤®‡≥ç‡≤∏‡≤∞‡≥ç‡≤ü‡≥ç‡≤™‡≥Ä‡≤™‡≤≤‡≥ç‡≤ó‡≥Ü‡≤ü‡≥ç‡≤è‡≤Ü‡≤®‡≤∏‡≥ç‡≤µ‡≤∞‡≥ç  \n3      ‡≤¨‡≤∞‡≤ø‡≤™‡≥ç‡≤∞‡≤Ø‡≥ã‡≤ú‡≤®‡≤µ‡≥á‡≤¨‡≤∞‡≤¶‡≤´‡≤ø‡≤≤‡≥ç‡≤Æ‡≤∏‡≥ç‡≤®‡≥ã‡≤°‡≥ä‡≤¶‡≥Å‡≤´‡≥ç‡≤Ø‡≤æ‡≤∏‡≤ø‡≤ï‡≥ã‡≤∞‡≥ç‡≤Ø‡≥Å‡≤ü‡≥ç‡≤Ø‡≥Ç‡≤¨‡≥ç...  \n4                     ‡≤é‡≤≤‡≥ç‡≤≤‡≤®‡≥Ç‡≤ú‡≥Ü‡≤¨‡≥ç‡≤ó‡≥Ü‡≤Ø‡≤æ‡≤ï‡≥ç‡≤ï‡≤Ç‡≤™‡≥á‡≤∞‡≥ç‡≤Æ‡≤æ‡≤°‡≥ç‡≤§‡≤ø‡≤Ø‡≤æ‡≤ó‡≥Å‡≤∞‡≥Å  \n...                                                  ...  \n24994  ‡≤ü‡≥Ç‡≤Ü‡≤µ‡≤∞‡≥ç‡≤∏‡≥Ü‡≤Ç‡≤ü‡≤ø‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç‡≤®‡≤æ‡≤ü‡≤ö‡≥ç‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≤æ‡≤®‡≥ç‡≤®‡≥Ä‡≤∞‡≥ç‡≤π‡≤æ‡≤ï‡≥ç‡≤∏‡≤ø‡≤Ö‡≤¶‡≤®‡≤ï‡≤µ‡≤∞‡≥ç...  \n24995                           ‡≤∏‡≥Ç‡≤™‡≤∞‡≥ç‡≤Ö‡≤®‡≥ç‡≤®‡≤æ‡≤∏‡≤ï‡≤§‡≤æ‡≤ó‡≤ø‡≤â‡≤ó‡≥ç‡≤¶‡≤ø‡≤°‡≤ø‡≤∞  \n24996  ‡≤ï‡≤®‡≥ç‡≤®‡≤°‡≤¶‡≤≤‡≥ç‡≤≤‡≤ø‡≤Æ‡≤æ‡≤°‡≥ã‡≤¨‡≤¶‡≥ç‡≤≤‡≥Å‡≤π‡≤ø‡≤Ç‡≤¶‡≤ø‡≤®‡≤≤‡≥ç‡≤≤‡≤ø‡≤Æ‡≤æ‡≤°‡≤ø‡≤¶‡≥ç‡≤∞‡≥Ü‡≤∏‡≥Å‡≤™‡≤∞‡≥ç‡≤∞‡≤ø‡≤ü‡≥ç...  \n24997  ‡≤è‡≤ú‡≤®‡≤§‡≤æ‡≤∂‡≤§‡≥ç‡≤∞‡≥Å‡≤ï‡≥Ç‡≤°‡≤æ‡≤ü‡≤ø‡≤ï‡≥ç‡≤ü‡≤æ‡≤ï‡≥ç‡≤®‡≤æ‡≤Æ‡≤æ‡≤§‡≥ç‡≤∞‡≤æ‡≤Ø‡≥Ç‡≤®‡≤ø‡≤®‡≥ç‡≤∏‡≥ç‡≤ü‡≤≤‡≥ç‡≤Æ‡≤¶‡≥ã‡≤≥‡≤Ö...  \n24998                 ‡≤Æ‡≤æ‡≤∞‡≥ç‡≤ï‡≥Ü‡≤ü‡≤ø‡≤Ç‡≤ó‡≥ç‡≤∏‡≥ç‡≤µ‡≤≤‡≥ç‡≤™‡≤¨‡≥á‡≤ï‡≤ø‡≤§‡≥ç‡≤§‡≥Å‡≤Ö‡≤®‡≥ç‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≥Ü  \n\n[24999 rows x 4 columns]\nTraining dataset:\n        index  labels                                             tweets\n0          0     2.0                   ‡≤Æ‡≤ï‡≥ç‡≤∏‡≥ç‡≤®‡≥ã‡≤°‡≤ø‡≤≤‡≥ç‡≤≤‡≤æ‡≤Ö‡≤¶‡≤ï‡≥ç‡≤ï‡≥Ü‡≤®‡≤ø‡≤Ç‡≤ó‡≥Ü‡≤á‡≤∑‡≥ç‡≤ü‡≤Ü‡≤ó‡≤ø‡≤≤\n1          1     4.0                                                ‡≥Å‡≤≥‡≥ç\n2          2     1.0                 ‡≤µ‡≤Æ‡≤æ‡≤ú‡≤®‡≥Ü‡≤¨‡≥Ü‡≤∞‡≥Ü‡≤Æ‡≤ø‡≤®‡≥ç‡≤∏‡≤∞‡≥ç‡≤ü‡≥ç‡≤™‡≥Ä‡≤™‡≤≤‡≥ç‡≤ó‡≥Ü‡≤ü‡≥ç‡≤è‡≤Ü‡≤®‡≤∏‡≥ç‡≤µ\n3          4     1.0  ‡≤ú‡≤®‡≤µ‡≥á‡≤¨‡≤∞‡≤¶‡≤´‡≤ø‡≤≤‡≥ç‡≤Æ‡≤∏‡≥ç‡≤®‡≥ã‡≤°‡≥ä‡≤¶‡≥Å‡≤´‡≥ç‡≤Ø‡≤æ‡≤∏‡≤ø‡≤ï‡≥ã‡≤∞‡≥ç‡≤Ø‡≥Å‡≤ü‡≥ç‡≤Ø‡≥Ç‡≤¨‡≥ç‡≤≤‡≤ø‡≤ï‡≤æ‡≤Æ‡≥Ü‡≤Ç‡≤ü...\n4          5     2.0                           ‡≤¨‡≥ç‡≤ó‡≥Ü‡≤Ø‡≤æ‡≤ï‡≥ç‡≤ï‡≤Ç‡≤™‡≥á‡≤∞‡≥ç‡≤Æ‡≤æ‡≤°‡≥ç‡≤§‡≤ø‡≤Ø‡≤æ‡≤ó‡≥Å\n...      ...     ...                                                ...\n24994  29994     3.0  ‡≤Ç‡≤ü‡≤ø‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç‡≤®‡≤æ‡≤ü‡≤ö‡≥ç‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≤æ‡≤®‡≥ç‡≤®‡≥Ä‡≤∞‡≥ç‡≤π‡≤æ‡≤ï‡≥ç‡≤∏‡≤ø‡≤Ö‡≤¶‡≤®‡≤ï‡≤µ‡≤∞‡≥ç‡≤´‡≥ã‡≤ü‡≥ã‡≤Æ‡≤æ‡≤°‡≤ø...\n24995  29996     4.0                                     ‡≤®‡≤æ‡≤∏‡≤ï‡≤§‡≤æ‡≤ó‡≤ø‡≤â‡≤ó‡≥ç‡≤¶‡≤ø‡≤°\n24996  29997     4.0          ‡≤≤‡≤ø‡≤Æ‡≤æ‡≤°‡≥ã‡≤¨‡≤¶‡≥ç‡≤≤‡≥Å‡≤π‡≤ø‡≤Ç‡≤¶‡≤ø‡≤®‡≤≤‡≥ç‡≤≤‡≤ø‡≤Æ‡≤æ‡≤°‡≤ø‡≤¶‡≥ç‡≤∞‡≥Ü‡≤∏‡≥Å‡≤™‡≤∞‡≥ç‡≤∞‡≤ø‡≤ü‡≥ç‡≤Ö‡≤ó‡≥ã\n24997  29998     3.0  ‡≤∞‡≥Å‡≤ï‡≥Ç‡≤°‡≤æ‡≤ü‡≤ø‡≤ï‡≥ç‡≤ü‡≤æ‡≤ï‡≥ç‡≤®‡≤æ‡≤Æ‡≤æ‡≤§‡≥ç‡≤∞‡≤æ‡≤Ø‡≥Ç‡≤®‡≤ø‡≤®‡≥ç‡≤∏‡≥ç‡≤ü‡≤≤‡≥ç‡≤Æ‡≤¶‡≥ã‡≤≥‡≤Ö‡≤®‡≥ç‡≤∏‡≥Å‡≤§‡≥Ü‡≤ï‡≥Ü...\n24998  29999     3.0                           ‡≤Ç‡≤ó‡≥ç‡≤∏‡≥ç‡≤µ‡≤≤‡≥ç‡≤™‡≤¨‡≥á‡≤ï‡≤ø‡≤§‡≥ç‡≤§‡≥Å‡≤Ö‡≤®‡≥ç‡≤∏‡≥Å‡≤§‡≥ç\n\n[24999 rows x 3 columns]\nTraining dataset:\n       index  labels                                             tweets\n0         0       0                                         00‡≤°‡≥á‡≤∏‡≥ç‡≤™‡≤ï‡≥ç‡≤ï\n1         3       0  ‡≤æ‡≤∞‡≤ø‡≤®‡≥Ä‡≤µ‡≥Å‡≤µ‡≥Ä‡≤°‡≤ø‡≤Ø‡≥ã‡≤®‡≤æ‡≤∞‡≥ã‡≤∏‡≥ç‡≤ü‡≥ç‡≤Æ‡≤æ‡≤°‡≤ø‡≤Ö‡≤¶‡≥ç‡≤∞‡≥Ü‡≤Æ‡≤¶‡≥ç‡≤µ‡≤™‡≤¨‡≥ç‡≤ó‡≥ç‡≤Ü‡≤ü‡≤µ‡≤æ‡≤´‡≥ç‡≤∞...\n2         4       0                                  ‡≥É‡≤∑‡≤£‡≤∂‡≤æ‡≤™‡≤§‡≤Ø‡≥ç‡≤§‡≥Ü‡≤≤‡≥Ä‡≤¨‡≥á‡≤ï‡≥Å\n3         6       0  ‡≤®‡≥ç‡≤®‡≤°‡≤á‡≤µ‡≤§‡≥ç‡≤§‡≥Å‡≤∞‡≤æ‡≤∑‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Ø‡≤Æ‡≤§‡≥ç‡≤§‡≥Å‡≤Ö‡≤Ç‡≤§‡≤æ‡≤∞‡≤æ‡≤∑‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Ø‡≤Æ‡≤ü‡≥ç‡≤ü‡≤¶‡≤≤‡≥ç‡≤≤‡≤ø...\n4         7       0  ‡≥á‡≤≥‡≤ø‡≤ï‡≤æ‡≤¶‡≤ø‡≤∞‡≥Å‡≤µ‡≤≠‡≤æ‡≤Ç‡≤¶‡≤µ‡≤∞‡≥á‡≤®‡≥ç‡≤≠‡≥Å‡≤µ‡≤ø‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø‡≤Ö‡≤µ‡≤®‡≤Ö‡≤∞‡≤ø‡≤§‡≤µ‡≤∞‡≥Ü‡≤®‡≥ç‡≤Ø‡≤æ‡≤∞‡≤ø‡≤≤‡≥ç...\n...     ...     ...                                                ...\n1049    771       4                                       ‡≤®‡≤ó‡≥Å‡≤ï‡≤®‡≥ç‡≤®‡≤°‡≤∏‡≤æ‡≤ï‡≥Å\n1050    773       0  ‡≥ç‡≤ü‡≤∞‡≥ç‡≤ü‡≤ø‡≤™‡≥ç‡≤∏‡≥ç‡≤ü‡≤æ‡≤∞‡≥ç‡≤ü‡≥ç‡≤Æ‡≤æ‡≤°‡≤≤‡≥Å‡≤ï‡≥ç‡≤Ø‡≤æ‡≤™‡≤ø‡≤ü‡≤ø‡≤ó‡≥ç‡≤∞‡≥å‡≤§‡≥ç‡≤Æ‡≤æ‡≤°‡≤≤‡≥Å‡≤ü‡≤ø‡≤™‡≥ç‡≤∏‡≥ç...\n1051    774       2                  ‡≤∂‡≥ç‡≤Æ‡≤ø‡≤ï‡≤á‡≤¶‡≤®‡≥ç‡≤®‡≤®‡≥ã‡≤°‡≤ø‡≤•‡≤ø*‡≤è‡≤ä‡≤∞‡≥ç‡≤ï‡≤ø‡≤Ç‡≤¨‡≥á‡≤ï‡≥Åü§£ü§£ü§£ü§£ü§£\n1052    776       0                ‡≥ã‡≤°‡≥ç‡≤§‡≤æ‡≤π‡≥ã‡≤¶‡≥ç‡≤∞‡≥Ü‡≤∏‡≤æ‡≤µ‡≤ø‡≤∞‡≤æ‡≤∞‡≥Å‡≤≤‡≤ø‡≤∞‡≤ø‡≤ï‡≤≤‡≥ç‡≤µ‡≥Ä‡≤°‡≤ø‡≤Ø‡≥ã‡≤ó‡≤≥‡≥Å\n1053    777       0                ‡≤ñ‡≤§‡≥ç‡≤ü‡≥ç‡≤∞‡≥ã‡≤≤‡≥ç‡≤¨‡≥ç‡≤∞‡≤¶‡≤∞‡≥ç‡≤®‡≥ç‡≤¨‡≥ç‡≤∞‡≥Ü‡≥Ç‡≤à‡≤ó‡≥á‡≤Æ‡≥ç‡≤®‡≥á‡≤Æ‡≥ç‡≤π‡≥á‡≤≥‡≤ø\n\n[1054 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"train = train.append(val)\ntrain = train.drop('index', axis = 1)\nprint(train['labels'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:24:20.352174Z","iopub.execute_input":"2023-09-08T14:24:20.352633Z","iopub.status.idle":"2023-09-08T14:24:20.367266Z","shell.execute_reply.started":"2023-09-08T14:24:20.352596Z","shell.execute_reply":"2023-09-08T14:24:20.366331Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"0.0    5815\n1.0    5080\n2.0    5071\n4.0    5058\n3.0    5029\nName: labels, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(train, test_size = 0.1)\nprint(val['labels'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:24:20.368731Z","iopub.execute_input":"2023-09-08T14:24:20.369381Z","iopub.status.idle":"2023-09-08T14:24:20.466146Z","shell.execute_reply.started":"2023-09-08T14:24:20.369339Z","shell.execute_reply":"2023-09-08T14:24:20.464719Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0.0    560\n1.0    517\n4.0    517\n2.0    515\n3.0    497\nName: labels, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"models = []\ntokenizers = []\nfrom transformers import AutoTokenizer, AutoModel, BertTokenizer, XLMRobertaTokenizer\nmodel_names = [\n    'xlm-roberta-base',\n    'bert-base-multilingual-cased',\n    'ai4bharat/indic-bert',\n    'l3cube-pune/kannada-bert'\n    \n]\ntokenizers = [\n    XLMRobertaTokenizer.from_pretrained('xlm-roberta-base'),\n    BertTokenizer.from_pretrained('bert-base-multilingual-cased'),\n    AutoTokenizer.from_pretrained('ai4bharat/indic-bert'),\n    AutoTokenizer.from_pretrained(\"l3cube-pune/kannada-bert\")\n]\n\nfor name in model_names:\n    model = AutoModel.from_pretrained(name)\n    model.eval()\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:24:20.467623Z","iopub.execute_input":"2023-09-08T14:24:20.467987Z","iopub.status.idle":"2023-09-08T14:25:27.559480Z","shell.execute_reply.started":"2023-09-08T14:24:20.467952Z","shell.execute_reply":"2023-09-08T14:25:27.558473Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0b209d378b475e9a3bcd869f06e67a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4796cf77e7542cb997d8b805aaf5c38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eaa2f903098423d8952584d9e814441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f15027512949418bf1ae9409938a76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c0559f9f39456fb0b6fd79064502a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7192d175d947dabc0ea9990d0fd0ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)ve/main/spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c10a9867f54ab7ad2db333e502e980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3afc7c15f4439f8651d4e4c312abc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a70fa509460146b89f1c7449425fbf37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d3e3899dc040a196f4dda00cacd6ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021b5473c52649d7bf887708ffd39e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b47b3977d44ca5b5ad80f2505e4343"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a38c229ba9f4669a7426bde2ce08b1c"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9eb009ba8549899ce1f3f7cde21f3e"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.bias', 'sop_classifier.classifier.bias', 'sop_classifier.classifier.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias']\n- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beac2b13c2734ee3a5395b49cc342973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/951M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa853679eed049a8bfbd3546ea0d0a94"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at l3cube-pune/kannada-bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at l3cube-pune/kannada-bert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"lin_dim = np.sum([1024 if 'large' in name else 768 for name in model_names])\nlin_dim","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.561034Z","iopub.execute_input":"2023-09-08T14:25:27.561406Z","iopub.status.idle":"2023-09-08T14:25:27.572643Z","shell.execute_reply.started":"2023-09-08T14:25:27.561372Z","shell.execute_reply":"2023-09-08T14:25:27.571729Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"3072"},"metadata":{}}]},{"cell_type":"code","source":"n_models = len(models)\nn_models","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.574106Z","iopub.execute_input":"2023-09-08T14:25:27.574686Z","iopub.status.idle":"2023-09-08T14:25:27.672475Z","shell.execute_reply.started":"2023-09-08T14:25:27.574650Z","shell.execute_reply":"2023-09-08T14:25:27.671510Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"code","source":"for model in models:\n    for param in model.parameters():\n        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.674206Z","iopub.execute_input":"2023-09-08T14:25:27.675121Z","iopub.status.idle":"2023-09-08T14:25:27.685660Z","shell.execute_reply.started":"2023-09-08T14:25:27.675086Z","shell.execute_reply":"2023-09-08T14:25:27.684476Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.689852Z","iopub.execute_input":"2023-09-08T14:25:27.690682Z","iopub.status.idle":"2023-09-08T14:25:27.697378Z","shell.execute_reply.started":"2023-09-08T14:25:27.690646Z","shell.execute_reply":"2023-09-08T14:25:27.696345Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"label_mapping = {\n    'Not_offensive': 0, \n    'Offensive_Targeted_Insult_Other': 1, \n    'Offensive_Targeted_Insult_Group': 2, \n    'Offensive_Untargetede': 3, \n    'Offensive_Targeted_Insult_Individual': 4\n}\n\n# Collecting Text and Labels\ntrain_batch_sentences = list(train['tweets'])\ntrain_batch_labels =  [x for x in train['labels']]\ndev_batch_sentences = list(val['tweets'])\ndev_batch_labels =  [x for x in val['labels']]\ntest_batch_sentences = list(val['tweets'])\ntest_batch_labels =  [x for x in val['labels']]\n\n\n# Convert to Tensor\ntrain_encodings = [tokenizer(train_batch_sentences, padding = 'max_length', truncation = True, max_length = 512, return_tensors = \"pt\") for tokenizer in tokenizers]\ntrain_labels = torch.tensor(train_batch_labels)\ndev_encodings = [tokenizer(dev_batch_sentences, padding = 'max_length', truncation = True, max_length = 512, return_tensors = \"pt\") for tokenizer in tokenizers]\ndev_labels = torch.tensor(dev_batch_labels)\ntest_encodings = [tokenizer(test_batch_sentences, padding = 'max_length', truncation = True, max_length = 512, return_tensors = \"pt\") for tokenizer in tokenizers]\ntest_labels = torch.tensor(test_batch_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:25:27.698823Z","iopub.execute_input":"2023-09-08T14:25:27.699365Z","iopub.status.idle":"2023-09-08T14:27:09.740045Z","shell.execute_reply.started":"2023-09-08T14:25:27.699330Z","shell.execute_reply":"2023-09-08T14:27:09.738969Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass kannada_Offensive_Dataset(Dataset):\n    def __init__(self, encodings, labels = None):\n        self.encodings = encodings\n        self.labels = labels\n        self.n_models = len(encodings)\n\n    def __getitem__(self, idx):\n        item = {}\n        for i in range(self.n_models):\n            item.update({key+'_'+str(i): torch.tensor(val[idx]) for key, val in self.encodings[i].items()})\n        item['index'] = idx\n        if self.labels != None:\n            item['labels'] = torch.tensor(self.labels[idx])\n        else:\n            item['labels'] = torch.tensor(1)\n        return item\n\n    def __len__(self):\n        return len(self.encodings[0]['input_ids'])\n\n# Defining Datasets\ntrain_dataset = kannada_Offensive_Dataset(train_encodings, train_labels)\nlen(train_dataset)\ndev_dataset = kannada_Offensive_Dataset(dev_encodings, dev_labels)\ntest_dataset = kannada_Offensive_Dataset(test_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.741731Z","iopub.execute_input":"2023-09-08T14:27:09.742108Z","iopub.status.idle":"2023-09-08T14:27:09.753296Z","shell.execute_reply.started":"2023-09-08T14:27:09.742073Z","shell.execute_reply":"2023-09-08T14:27:09.751200Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\n\n# Basic Fully-Connected (Linear => BatchNorm => ReLU)\nclass BasicFC(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicFC, self).__init__()\n        self.fc = nn.Linear(in_channels, out_channels, **kwargs)\n        self.bn = nn.BatchNorm1d(out_channels, eps = 0.001)\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = self.bn(x)\n        return F.relu(x, inplace = True)\n\nclass FusionNet(torch.nn.Module):\n    def __init__(self, D_in, H1, H2, H3, H4, D_out):\n        super(FusionNet, self).__init__()\n        self.linear1_1 = BasicFC(D_in, H1)\n        self.linear1_2 = BasicFC(H1, H2)\n        self.linear1_3 = BasicFC(H2, H3)\n        self.linear1_4 = BasicFC(H3, H4)\n        self.dp = nn.Dropout(0.1)\n        self.linear2 = torch.nn.Linear(H4, D_out, bias = False)\n\n    def forward(self, x):\n        h_relu_1 = self.linear1_1(x)\n        h_relu_2 = self.dp(self.linear1_2(h_relu_1))\n        h_relu_3 = self.dp(self.linear1_3(h_relu_2))\n        h_relu_4 = self.dp(self.linear1_4(h_relu_3))\n        y_pred = self.linear2(h_relu_4)\n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.754601Z","iopub.execute_input":"2023-09-08T14:27:09.755032Z","iopub.status.idle":"2023-09-08T14:27:09.770502Z","shell.execute_reply.started":"2023-09-08T14:27:09.754996Z","shell.execute_reply":"2023-09-08T14:27:09.769423Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\n## Loss Fn\nXE_loss_function = nn.CrossEntropyLoss(reduction = 'mean').float()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.771898Z","iopub.execute_input":"2023-09-08T14:27:09.772271Z","iopub.status.idle":"2023-09-08T14:27:09.794203Z","shell.execute_reply.started":"2023-09-08T14:27:09.772234Z","shell.execute_reply":"2023-09-08T14:27:09.793267Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class F1_Loss(nn.Module):\n    def __init__(self, epsilon = 1e-7, n_labels = 5):\n        super().__init__()\n        self.epsilon = epsilon\n        self.n_labels = n_labels\n        \n    def forward(self, y_pred, y_true,):\n        assert y_pred.ndim == 2\n        assert y_true.ndim == 1\n        y_true = F.one_hot(y_true, self.n_labels).to(torch.float32)\n        y_pred = F.softmax(y_pred, dim = 1)\n        \n        tp = (y_true * y_pred).sum(dim = 0).to(torch.float32)\n        tn = ((1 - y_true) * (1 - y_pred)).sum(dim = 0).to(torch.float32)\n        fp = ((1 - y_true) * y_pred).sum(dim = 0).to(torch.float32)\n        fn = (y_true * (1 - y_pred)).sum(dim = 0).to(torch.float32)\n\n        precision = tp / (tp + fp + self.epsilon)\n        recall = tp / (tp + fn + self.epsilon)\n\n        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n        f1 = f1.clamp(min = self.epsilon, max = 1 - self.epsilon)\n        return 1 - f1.mean()\n\nF1_loss_function = F1_Loss().cuda()\n\nuse_f1_loss = False\nif use_f1_loss:\n    loss_function = F1_loss_function\nelse:\n    loss_function = XE_loss_function","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.795672Z","iopub.execute_input":"2023-09-08T14:27:09.796014Z","iopub.status.idle":"2023-09-08T14:27:09.807418Z","shell.execute_reply.started":"2023-09-08T14:27:09.795981Z","shell.execute_reply":"2023-09-08T14:27:09.806390Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"add_extra_embeds = [\n    'cnn_128',\n    #'sentbert',\n    #'laser_ta',\n    #'laser_en',\n]\n\nembeds_files = {\n    'cnn_128': ['/kaggle/input/embeddings/cnn_emb_train_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_test_128_kannada.npy'],\n     'laser_kn': ['/kaggle/input/embeddings/cnn_emb_train_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_test_128_kannada.npy'],\n     'laser_en': ['/kaggle/input/embeddings/cnn_emb_train_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_test_128_kannada.npy'],\n     'sentbert': ['/kaggle/input/embeddings/cnn_emb_train_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy', '/kaggle/input/embeddings/cnn_emb_dev_128_kannada.npy']\n}\n\ntrain_embeddings = [np.load(embeds_files[embname][0]) for embname in add_extra_embeds]\ndev_embeddings = [np.load(embeds_files[embname][1]) for embname in add_extra_embeds]\ntest_embeddings = [np.load(embeds_files[embname][2]) for embname in add_extra_embeds]\ntorch.Tensor(train_embeddings).shape","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:09.808798Z","iopub.execute_input":"2023-09-08T14:27:09.809158Z","iopub.status.idle":"2023-09-08T14:27:10.031481Z","shell.execute_reply.started":"2023-09-08T14:27:09.809124Z","shell.execute_reply":"2023-09-08T14:27:10.030452Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 4695, 128])"},"metadata":{}}]},{"cell_type":"code","source":"len_extra_embeds = len(add_extra_embeds)\nprint(len_extra_embeds)\n\ndim_extra_embeds = np.sum([train_embeddings[i].shape[1] for i in range(len_extra_embeds)])\nprint(dim_extra_embeds)\n\nprint(lin_dim + dim_extra_embeds)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:10.033014Z","iopub.execute_input":"2023-09-08T14:27:10.033449Z","iopub.status.idle":"2023-09-08T14:27:10.039997Z","shell.execute_reply.started":"2023-09-08T14:27:10.033414Z","shell.execute_reply":"2023-09-08T14:27:10.038773Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1\n128\n3200\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import classification_report, f1_score\n\nfusion_classifier = FusionNet(int(3200), int(3200), 1024, 256, 64, 5)\n# Optimiser\noptimizer = AdamW(fusion_classifier.parameters(), lr = 0.0001)\nmo,ex_em = \"\",\"\"\nfor mod in model_names:\n    mo += mod\nmodel_name = 'fusion_kannada_'+ mo\n\ndevice = torch.device('cuda')\nfusion_classifier.to(device)\nfor model in models:\n    model.to(device)\nbest_val_f1 = 0\ncount = 0\n\n# Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size = 10, shuffle = True)\ndev_loader = DataLoader(dev_dataset, batch_size = 10, shuffle = False)\ntest_loader = DataLoader(test_dataset, batch_size = 10, shuffle = False)\nPATH = \"\"\nfor epoch in range(100):\n    fusion_classifier.train()\n    print(\"==========================================================\")\n    print(\"Epoch {}\".format(epoch))\n    print(\"Train\")\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        outputs_all = []\n        for i in range(n_models):\n            model = models[i]\n            input_ids = batch['input_ids' + '_' + str(i)].to(device)\n            attention_mask = batch['attention_mask' + '_' + str(i)].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask = attention_mask)\n            outputs_all.append(outputs[1])\n        \n        for i in range(len_extra_embeds):\n            print(torch.Tensor(train_embeddings[i].shape))\n            print(batch['index'])\n            outputs_all.append(torch.Tensor(train_embeddings[i][batch['index'], :]).to(device))\n            \n        bert_output = torch.cat(outputs_all, dim = -1) \n        out = fusion_classifier(bert_output)\n        loss = loss_function(out, labels)\n        loss.backward()\n        optimizer.step()\n    \n    print(\"Dev\")\n    dev_preds,dev_probs = [],[]\n    fusion_classifier.eval()\n    total_val_loss = 0\n    with torch.set_grad_enabled(False):\n        for batch in tqdm(dev_loader):\n            outputs_all = []\n            for i in range(n_models):\n                model = models[i]\n                input_ids = batch['input_ids' + '_' + str(i)].to(device)\n                attention_mask = batch['attention_mask' + '_' + str(i)].to(device)\n                labels = batch['labels'].to(device)\n                outputs = model(input_ids, attention_mask = attention_mask)\n                outputs_all.append(outputs[1])\n                \n            for i in range(len_extra_embeds):\n                outputs_all.append(torch.Tensor(dev_embeddings[i][batch['index'], :]).to(device))\n\n            bert_output = torch.cat(outputs_all, dim = -1) \n            out = fusion_classifier(bert_output)\n            loss = loss_function(out, labels)\n            total_val_loss += loss.item()/len(dev_loader)\n            \n            for logits in out.cpu().numpy():\n                dev_preds.append(np.argmax(logits))\n            for logits in out.cpu().numpy():\n                dev_probs.append(np.exp(logits)/np.sum(np.exp(logits)))\n    \n    y_true = dev_batch_labels\n    y_pred = dev_preds\n    target_names = label_mapping.keys()\n    report = classification_report(y_true, y_pred, target_names=target_names)\n    val_f1 = f1_score(y_true, y_pred, average = 'macro')\n    \n    if val_f1 > best_val_f1:\n        PATH = '/kaggle/working/'\n#         torch.save(fusion_classifier.state_dict())\n        best_val_f2 = val_f1\n        \n        #save dev preds\n#         with open('/kaggle/working/', 'wb') as f:\n#             print(np.array(dev_probs).shape)\n#             np.save(f + str(model_name) + '.npy', np.array(dev_probs))\n            \n        #find test preds\n        print(\"Test\")\n        test_preds,test_probs = [],[]\n        fusion_classifier.eval()\n        total_test_loss = 0\n        with torch.set_grad_enabled(False):\n            for batch in tqdm(test_loader):\n                outputs_all = []\n                for i in range(n_models):\n                    model = models[i]\n                    input_ids = batch['input_ids' + '_' + str(i)].to(device)\n                    attention_mask = batch['attention_mask' + '_' +str(i)].to(device)\n                    labels = batch['labels'].to(device)\n                    outputs = model(input_ids, attention_mask=attention_mask)\n                    outputs_all.append(outputs[1])\n\n                for i in range(len_extra_embeds):\n                    outputs_all.append(torch.Tensor(test_embeddings[i][batch['index'], :]).to(device))\n\n                bert_output = torch.cat(outputs_all, dim = -1) \n                out = fusion_classifier(bert_output)\n                loss = loss_function(out, labels)\n                total_test_loss += loss.item()/len(test_loader)\n\n                for logits in out.cpu().numpy():\n                    test_preds.append(np.argmax(logits))\n                for logits in out.cpu().numpy():\n                    test_probs.append(np.exp(logits)/np.sum(np.exp(logits)))\n\n        y_pred = test_preds\n        target_names = label_mapping.keys()\n#         with open('/kaggle/working/', 'wb') as f:\n#             print(np.array(test_probs).shape)\n#             np.save(f + str(model_name)+'.npy', np.array(test_probs))\n            \n        best_val_f1 = val_f1\n        count = 0\n    else:\n        count += 1\n    \n    print(report)\n    print(\"Epoch {}, Val Loss = {}, Val F1 = {}, Best Val f1 = {}, stagnant_t = {}\".format(epoch, total_val_loss, val_f1, best_val_f1, count))\n    if count == 5:\n        print(\"No increase for 5 epochs, Stopping ...\")\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:19.999597Z","iopub.status.idle":"2023-09-08T14:27:20.000078Z","shell.execute_reply.started":"2023-09-08T14:27:19.999834Z","shell.execute_reply":"2023-09-08T14:27:19.999858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import classification_report, f1_score\n\ndevice = torch.device('cuda')\nmodel.to(device)\n\n# Dataloaders\ndev_loader = DataLoader(dev_dataset, batch_size = 16, shuffle = False)\n# fusion_classifier.load_state_dict(torch.load('/kaggle/working/' + str(model_name) + '.pth'))\nfusion_classifier.eval()\n\ndev_preds = []\nwith torch.set_grad_enabled(False):\n    for batch in tqdm(dev_loader):\n        outputs_all = []\n        for i in range(n_models):\n            model = models[i]\n            input_ids = batch['input_ids'+'_'+str(i)].to(device)\n            attention_mask = batch['attention_mask'+'_'+str(i)].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            outputs_all.append(outputs[1])\n            \n        for i in range(len_extra_embeds):\n            outputs_all.append(torch.Tensor(dev_embeddings[i][batch['index'], :]).to(device))\n\n        bert_output = torch.cat(outputs_all, dim = -1) \n        out = fusion_classifier(bert_output)\n            \n        for logits in out.cpu().numpy():\n            dev_preds.append(np.argmax(logits))","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:20.002173Z","iopub.status.idle":"2023-09-08T14:27:20.002691Z","shell.execute_reply.started":"2023-09-08T14:27:20.002433Z","shell.execute_reply":"2023-09-08T14:27:20.002457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = dev_batch_labels\ny_pred = dev_preds\ntarget_names = label_mapping.keys()\nreport = classification_report(y_true, y_pred, target_names = target_names)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:27:20.004404Z","iopub.status.idle":"2023-09-08T14:27:20.004875Z","shell.execute_reply.started":"2023-09-08T14:27:20.004637Z","shell.execute_reply":"2023-09-08T14:27:20.004660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}